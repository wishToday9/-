# C++

## 基础知识

* **C++内存分区**

 1. 堆区

    用于存储动态分配内存的区域，通常需要手动分配和释放，调用new/delete

 2. 栈区

    用于存储局部变量和函数调用信息的区域，栈上的变量的生命周期和所在函数执行周期对应，函数调用结束时，栈上的变量会自动销毁。

3. 全局区

​		用于存储全局变量和静态变量的区域，全局变量在整个程序的执行期间都存在，生命周期从程序启动到中止，动态变量在其作用域内存在，生命周期也是如此。

4. 常量区

   常量区是用来存储常量的区域，如字符串类常量，全局常量

5. 代码区

​		代码区是用于存储程序的机器指令的内存区域。这是程序的可执行部分，通常在程序启动时加载到内存中。代码区的数据是只读的，不应进行写操作。



* **C和C++的区别是什么？**

1. C是面向过程的编程语言，C++是面向对象的编程语言，因此C++语言中有继承、多态，此外C++支持模板、异常处理机制以及C++ STL库。
2. 动态内存管理。C语言使用malloc/free,C++使用new/delete
3. 强制类型转换不一样。C语言直接()强转，C++提供了四种类型的强制转换方式(static_cast、const_cast、reinterpret_cast、dynamic_cast)。
4. 异常捕获机制。C语言不提供异常处理机制，错误通常以返回值或者全局变量的形式进行处理，C++支持异常处理，允许程序员自己编写更健壮的代码处理异常。
5. C++还有引用、智能指针、auto关键字等等。



* **C++四种类型转换**

1. 静态转换static_cast

   这是最常见的类型转换操作，用于显式将一个数据类型转换为另一个数据类型，通常用于基本类型的转换、父子类之间的转换，静态转换是在编译期间执行的，不提供运行检查，因此使用的时候要慎用。

2. 动态类型转换dynamic_cast

   通常用于处理多态继承结构中的类型转换，例如基类指针或者引用上进行类型检查和转换，动态转换在运行期间执行，同时会检查是否可以安全的转换，如果不能安全转换，则会返回空指针(用于指针的情况)或者抛出异常(用于引用的情况)。

   ```cpp
   Base* basePtr = new Derived;
   Derived* derivedPtr = dynamic_cast<Derived*>(basePtr);
   if (derivedPtr) {
       // 安全的类型转换
   } else {
       // 转换失败
   }
   ```

3. 常量转换const_cast

   常量转换用于移除和添加const限定符，允许对象在const和非const版本之间进行切换。

4. 重新解释转换reinterpret_Cast

​		重新解释转换是一种非常底层的转换，通常用于将一个指针或引用从一种类型转换为另一种类型。



* **dynamic_cast 什么情况下不能转换**

**无虚函数**：如果类层次结构中的基类没有虚函数，`dynamic_cast` 无法工作。因为 `dynamic_cast` 依赖于运行时类型信息（RTTI），而虚函数表（vtable）通常用于存储这些信息。如果基类没有虚函数，那么对象的类型信息就不会被存储，`dynamic_cast` 将无法获知对象的实际类型。

**没有派生关系**：如果两个类之间没有继承关系，`dynamic_cast` 无法进行转型。它只用于基类和派生类之间的类型转换。

**对象为空指针**：如果传递给 `dynamic_cast` 的对象指针是空指针，则转换也会失败。

* **dynamic_cast怎么实现的**







* **C++异常处理机制**

​	C++异常处理机制适用于一些程序中异常情况的机制。

1. 异常类别

   C++标准库定义了一组异常类别，如runtime_error、logic_error等，用于表示不同类型异常情况，我们也可以自定义异常类别，派生自std::exception类别

2. try

   `try` 块用于包裹可能引发异常的代码块。

3. catch

​		`catch` 块用于捕获和处理异常。你可以有多个 `catch` 块，每个块可以捕获不同类型的异常。`catch` 块中的代码会在异常抛出时执行。

4. throw

  	`throw` 用于抛出异常，将控制流转移到匹配的 `catch` 块。你可以抛出异常对象，这些对象可以是内置数据类型、自定义类对象或标准库异常类的对象。



* **struct和class区别**

1. struct一般用于数据结构的集合，class是对一个对象数据的封装
2. struct默认的访问权限是public，class默认访问权限是private
3. 继承关系中，struct默认是公有继承，class默认是私有继承】

补充了一下C/C++中struct的不同

|          |           C            | C++                      |
| :------: | :--------------------: | ------------------------ |
| 成员函数 |         不能有         | 可以                     |
| 静态成员 |         不能有         | 可以                     |
| 访问控制 |  默认public，不能修改  | public/private/protected |
| 继承关系 |       不可以继承       | 可从类或者其他结构体继承 |
|  初始化  | 不能直接初始化数据成员 | 可以                     |



* 导入C函数的关键字，C++编译时和C有什么不同？

1. **关键字：**在C++中，导入C函数的关键字是**extern**，表达形式为**extern “C”**， extern "C"的主要作用就是为了能够正确实现C++代码调用其他C语言代码。加上extern "C"后，会指示编译器这部分代码按**C语言**的进行编译，而不是C++的。
2. **编译区别：**由于C++支持函数重载，因此编译器编译函数的过程中会将函数的**参数类型**也加到编译后的代码中，而不仅仅是**函数名**；而C语言并不支持函数重载，因此编译C语言代码的函数时不会带上函数的参数类型，一般只包括**函数名**。



* **函数指针相关**

  概念：函数指针就是**指向函数**的指针变量。每一个函数都有一个入口地址，该入口地址就是函数指针所指向的地址。

  定义：

  ```cpp
  return_type (*pointer_name)(parameter_type1, parameter_type2, ...);
  ```

  应用场景：

  1. 回调函数

     允许将一个函数传递给另一个函数实现回调。

## 多态

* **什么是多态？C++如何实现多态？**

多态就是同一个函数名的多种状态或者是一个接口具有不同的行为，C++多态分为编译期多态和运行时多态，编译期多态成为静态联编，通过重载和模板实现，运行时多态成为动态联编，通过继承和虚函数实现。

* **什么是虚函数？什么是纯虚函数**

C++虚函数是一种用于实现多态性的机制，它允许在派生类中重写基类的函数，并且在运行的时候执行正确的函数版本，虚函数用于创建一个虚函数表，其中包含了正确的函数的指针。

1. 虚函数的定义

   在基类中，你可以将一个成员函数标记为虚函数，在函数声明前面加上关键字virtual。派生类可以重写虚函数，即在派生类中提供自己的实现。

2. 虚函数表

   每个带虚函数的类都会包含一个虚函数表，其中存储了指向虚函数的指针，这个表在程序运行的时候确定要调用的函数。派生类的虚函数包含的是指向派生类函数的指针，从而实现了多态性。

3. 纯虚函数

​		纯虚函数是一种特许类型的虚函数，他没有具体的实现，只有声明，要求派生类必须提供自己的实现。

* **虚函数机制**

- - 虚函数是通过虚函数表来实现的，虚函数表包含了一个类(所有)的虚函数的地址，在有虚函数的类对象中，它内存空间的头部会有一个虚函数表指针(虚表指针)，用来管理虚函数表。当子类对象对父类虚函数进行重写的时候，虚函数表的相应虚函数地址会发生改变，改写成这个虚函数的地址，当我们用一个父类的指针来操作子类对象的时候，它可以指明实际所调用的函数。
  - 如果子类中不存在虚函数，那么子类的虚指针将直接指向父类的虚函数表，如果子类中有虚函数，则会产生自己的虚函数表；如果没有将父类中的虚函数全部重载，那么没有被重载的虚函数指针将会指向父类虚函数的地址。

* **析构函数**

子类实现析构函数，使得在析构函数中释放子类所占用的资源，如果父类不声明为虚函数，那么会造成子类的资源未释放而导致内存泄漏。

* **钻石(菱形)继承存在什么问题，如何解决**？

存在二义性问题，两个父类对公共基类的数据和方法产生一份拷贝，因此对于子类来说的，读写一个公共基类的数据或者调用一个方法的时候，不知道是哪一个父类的数据和方法，就会导致编译出错。可以采用虚继承的方法解决这个问题，这样就只会创造一份公共基类的实例，不会造成二义性。

* **C++ RTTI 机制**

一般情况下，在编译期间就能确定一个表达式的类型，但是当存在多态时，有些表达式的类型在编译期间就无法确定了，必须等到程序运行后根据实际的环境来确定。

C++内存模型主要包含以下内容：

1. 如果没有虚函数也没有虚继承，那么对象内存模型只有成员变量。
2. 如果包含了虚函数，那么会额外添加一个虚函数表，并在对象内存中插入一个指针，只想虚函数表。
3. 如果包含了虚继承，那么额外添加一个虚基类表，并且在对象内存中插入一个指针，指向这个虚基类表。

编译器会在虚函数表 vftable 的开头插入一个指针，指向当前类对应的 type_info 对象。当程序在运行阶段获取类型信息时，可以通过对象指针 p 找到虚函数表指针 vfptr，再通过 vfptr 找到 type_info 对象的指针，进而取得类型信息。

![image-20231205234029598](C++.assets\image-20231205234029598.png)

* **C++ RAII（**R**esource **A**cquisition **I**s **I**nitialization）**

资源的使用一般经历三个步骤a.获取资源 b.使用资源 c.销毁资源，但是资源的销毁往往是程序员经常忘记的一个环节，所以程序界就想如何在程序员中让资源自动销毁呢？c++之父给出了解决问题的方案：RAII，它充分的利用了C++语言局部对象自动销毁的特性来控制资源的生命周期。给一个简单的例子来看下局部对象的自动销毁的特性：

```cpp
#include <iostream>
using namespace std;
class person {
  public:
      person(const std::string name = "", int age = 0) : 
      name_(name), age_(age) {
            std::cout << "Init a person!" << std::endl;
      }
      ~person() {
            std::cout << "Destory a person!" << std::endl;
      }
      const std::string& getname() const {
            return this->name_;
      }    
      int getage() const {
            return this->age_;
      }      
  private:
      const std::string name_;
      int age_;  
};
int main() {
    person p;
    return 0;
}
编译并运行：
g++ person.cpp -o person
./person 
运行结果：
Init a person!
Destory a person!
```

从person class可以看出，当我们在main函数中声明一个局部对象的时候，会自动调用构造函数进行对象的初始化，当整个main函数执行完成后，自动调用析构函数来销毁对象，整个过程无需人工介入，由操作系统自动完成；于是，很自然联想到，当我们在使用资源的时候，在构造函数中进行初始化，在析构函数中进行销毁。整个RAII过程我总结四个步骤：

a.设计一个类封装资源

b.在构造函数中初始化

c.在析构函数中执行销毁操作

d.使用时声明一个该对象的类

**shared_ptr与weak_ptr是典型的应用案例。**

## 内存管理

* **C++有哪些内存区域**

​	（1）堆，使用malloc、free动态分配和释放空间，能分配较大的内存；

​	（2）栈，为函数的局部变量分配内存，能分配较小的内存；

​	（3）全局/静态存储区，用于存储全局变量和静态变量；

​	（4）常量存储区，专门用来存放常量；

​	（5）代码区；

* **堆区和栈区有哪些区别**
  * 堆中的内存需要手动申请和手动释放，栈中的内存是由OS自动申请和自动释放的。
  * 堆能分配的内存的内存比较大（GB级别），栈能分配的内存比较小（MB级别）。
  * 在堆中分配和释放内存会产生内存碎片，栈不会产生内存碎片。
  * 堆的分配效率比较低下，栈的分配效率比较高。
  * 堆地址从低到高，栈地址从高到底。



* **什么是内存对齐，为什么要做内存对齐，怎么实现的？**

  * （1）内存对齐的原因：关键在于CPU存取数据的效率问题。为了提高效率，计算机从内存中取数据是按照一个固定长度的。比如在32位机上，CPU每次都是取32bit数据的，也就是4字节；若不进行对齐，要取出两块地址中的数据，进行掩码和移位等操作，写入目标寄存器内存，效率很低。内存对齐一方面可以节省内存，一方面可以提升数据读取的速度；

  * （2）内容：内存对齐指的是C++结构体中的数据成员，其内存地址是否为其对齐字节大小的倍数。

  * （3）**对齐原则：**

    * 1）结构体变量的首地址能够被其最宽基本类型成员的对齐值所整除；

    - 2）结构体内每一个成员的相对于起始地址的偏移量能够被该变量的大小整除；

    - 3）结构体总体大小能够被最宽成员大小整除；如果不满足这些条件，编译器就会进行一个填充(padding)。
    - 4）如何对齐：声明数据结构时，字节对齐的数据依次声明，然后小成员组合在一起，能省去一些浪费的空间，不要把小成员参杂声明在字节对齐的数据之间。


* **malloc/free和new/delete的区别**

1. malloc/free是C语言的函数，而new/delete是C++语言的运算符。
2. 类型信息上，malloc返回一个void*指针，需要显示的类型转换，new返回指定类型的指针。
3. 构造函数和析构函数上，malloc和free知识分配和释放内存，不会调用对象的构造函数和析构函数，需要手动调用，new/delete会调用对象的构造函数来初始化对象，并且在释放内存的时候调用析构函数清理资源。
4. 异常处理，new在分配失败的时候直接会抛出std::bad_malloc异常，malloc分配失败直接返回NULL指针，需要手动检查。



* **调用new/delete具体发生了什么？**

​		1.调用new

​		①分配足够的内存以容纳对象的大小

​		②调用构造函数来初始化对象的状态

​		③返回指向分配内存的指针

​		2. 调用delete

​		①调用析构函数来清理对象的状态

​		②释放对象占用的内存



## 指针

* **什么是野指针，怎么产生的，如何避免**

概念：野指针指的是指针指向的位置是不可知的（不正确的、随机的）

产生原因：释放内存后指针没有置空，依然指向了该内存，那么就可能出现非法访问的错误

避免：

	1. 初始化置为nullptr
	1. 申请内存后判断空
	1. 释放内存后置为nullptr
	1. 使用智能指针

* **C++中的智能指针有哪些，各自有什么作用？**

智能指针主要是用来解决内存泄漏的问题，以及避免多个裸指针指向同一资源时，多次释放资源时，对悬空指针进行释放导致不可预知的错误，它可以主动释放内存，因为它本身是一个类，在函数结束的时候调用析构函数释放内存，智能指针分为不带引用计数的unique_ptr，带引用计数的shared_ptr和weak_ptr

1. unique_ptr删除了拷贝构造函数和赋值函数，因此不支持普通的拷贝或赋值操作。但引入了移动构造函数和移动赋值运算符。所以它们保证了有唯一的智能指针持有此资源。unique_ptr还提供了reset重置资源，swap交换资源等函数，也经常会使用到。

2. shared_ptr称为强智能指针，**它的资源引用计数器在内存的heap堆上**这保证了，每个智能指针的引用计数变量会动态的变化）。**通常用于管理对象的生命周期**。只要有一个指向对象的shared_ptr存在，该对象就不会被析构。

3. weak_ptr被称为弱智能指针，其对资源的引用**不会引起资源的引用计数的变化**，通常作为观察者，用于判断资源是否存在，并根据不同情况做出相应的操作。比如使用weak_ptr对资源进行弱引用，当调用weak_ptr的lock()方法时，若返回nullptr，则说明资源已经不存在，放弃对资源继续操作。否则，将返回一个shared_ptr对象，可以继续操作资源。另外，一旦最后一个指向对象的shared_ptr被销毁，对象就会被释放。即使有weak_ptr指向对象，对象也还是会被释放

* **shared_ptr之间的相互引用问题**

![image-20231211202531146](C++.assets\image-20231211202531146.png)



* **shared_ptr的实现原理是什么？构造函数、拷贝构造函数和赋值运算符怎么写？shared_ptr是不是线程安全的？**

1. shared_ptr是通过引用计数机制实现的，引用计数存储着有几个shared_ptr指向相同的对象，当引用计数下降至0时就会自动销毁这个对象；
2. 具体实现：
   1. 构造函数：将指针指向该对象，引用计数置为1
   2. 拷贝构造函数：将指针指向该对象，引用计数++
   3. 赋值运算符号：=号左边的shared_ptr的引用计数-1，右边的shared_ptr引用计数+1，如果左边的引用计数降为了0，要销毁对象并释放内存空间。
3. shared_ptr的引用计数本身是安全且无锁的，但是它指向的对象的读写则不是，因此可以说shared_ptr不是线程安全的。



* **指针和引用的区别**

1. 指针本质上是一个地址，有自己的内存空间，引用只是一个别名。
2. 指针可以指向其他的对象，但是引用不能指向其他的对象，初始化之后就不能变了。
3. 指针可以初始化为nullptr，而引用必须被初始化为一个已有对象的引用。
4. 指针可以是多级指针，引用只能是一级。
5. 引用实质上是一个指针常量。



* **Delete和Delete[]的区别，delete[]如何知道要delete多少次，在类的成员函数中能否Delete This？**

（1）若是基本类型，delete和delete[]效果是一样的，因为系统会自动记录分配的空间，然后释放；对于自定义数据类型而言（比如类）就不行了，delete仅仅释放数组第一个元素的内存空间，且仅调用了第一个对象的析构函数，但delete[]会调用数组所有元素的析构函数，并释放所有内存空间；

（2）这个问题直接导致我们需要在new []一个对象数组时，需要保存数组的维度，C++的做法是在分配数组空间时多分配了4个字节的大小，专门保存数组的大小，这个数据应该就存在这个分配返回的指针周围，在 delete[]时就可以取出这个保存的数，就知道了需要调用析构函数多少次了；

（3）在类的成员函数可以调用delete this，并且delete this之后还可以调用该对象的其他成员，但是有个前提：被调用的方法不涉及这个对象的数据成员和虚函数。当一个类对象声明时，系统会为其分配内存空间。在类对象的内存空间中，只有数据成员和虚函数表指针，并不包含代码内容，类的成员函数单独放在代码段中。

* **C++ unique_ptr和裸指针的性能开销上有什么不同？**

  * **内存管理开销：** `std::unique_ptr` 通常包含一个指向堆上对象的指针和一个额外的控制块，用于管理资源的所有权。这可能导致 `std::unique_ptr` 对象相较于裸指针占用更多的内存。然而，现代编译器通常会进行优化，使得这种开销相对较小。

  * **构造和销毁开销：** 创建和销毁 `std::unique_ptr` 对象可能涉及更多的工作，因为它需要在构造和析构时管理资源。相较之下，裸指针的构造和销毁开销较小。但这种差异通常在实际应用中不是性能瓶颈。

  * **函数调用开销：** 将 `std::unique_ptr` 传递给函数可能会涉及更多的指针复制，因为 `std::unique_ptr` 是非拷贝可传递（move-only）的。这意味着在函数调用中可能会执行资源所有权的转移。对比之下，裸指针的传递只涉及指针值的复制。

  * **异常处理开销：** `std::unique_ptr` 提供了异常安全性，当异常发生时，会自动释放其管理的资源。这可能导致与裸指针相比更多的开销。但这种开销通常是可以接受的，尤其是考虑到异常安全性的好处。

`std::unique_ptr` 在实现上通常包含一个控制块（control block），它用于管理所拥有对象的生命周期和资源释放。这个控制块包含了以下信息：

1. **指向动态分配的对象的指针：** 这是 `std::unique_ptr` 实际拥有的指针，指向通过 `new` 运算符分配的内存。
2. **指向删除器（deleter）的指针或函数：** 删除器是一个用于释放对象的函数或函数对象。当 `std::unique_ptr` 被销毁或通过 `reset` 释放对象时，控制块会使用删除器来正确释放资源。如果未提供删除器，则默认使用 `delete`。
3. **其他管理信息：** 可能包括引用计数、定制内存分配器等。

这个控制块的存在允许 `std::unique_ptr` 进行资源管理，确保在适当的时候释放动态分配的内存。这是与裸指针最大的区别之一，因为裸指针本身没有内建的资源管理机制。



## 现代C++

### 语言可用性强化

* 引入了nullptr、constexpr、auto、using等
* 在面向对象上，引入了继承构造、委托构造。

### 模板

* **引入了变长参数模板**

* **模板为什么声明和定义不能分离**

C++是分离式编译，编译是对每一个cpp文件而言的，将cpp编译成独立的obj，再将obj进行链接生成exe可执行程序，对于模板而言，模板只有被调用的时候才会被实例化，如果将声明和定义分开放，那么模板函数不会被实例化，就会发生链接错误。

* **类模板和模板类有什么区别**
  * 类模板是一个通用的模板，定义了一个类的框架，其中某些数据成员和方法可以在实例化的时候指定或推断，类模板是一种通用的定义。
  * 模板类是通过实例化而来的具体的类，即在使用时将模板参数替换为实际的类型。

* **函数模板(泛化和特化的问题)，什么是全特化，什么是偏特化**

  * 所谓模板全特化限定死模板实现的具体类型；
  * 偏特化是指提供另一份template定义式，而其本身仍为`templatized`，这是针对于`template`参数更进一步的条件限制所设计出来的一个特化版本。也就是如果这个模板有多个类型，那么**只限定其中的一部分**;

  ```cpp
  //模板全特化
  template<>
  class Test<int,int>
  {
  public:
   Test(int a, int b) :_a(a), _b(b)
   {
    cout << "模板全特化" << endl;
   }
  private:
   int _a;
   int _b;
  };
  
  //模板偏特化
  template<class T>
  class Test<int,T>
  {
  public:
   Test(int a, T b) :_a(a), _b(b)
   {
    cout << "模板偏特化" << endl;
   }
  private:
   int _a;
   T _b;
  };
  ```

  

### 语言运行期强化

#### Lambda表达式

* **Lambda表达式**

实际上就是提供了一个类似匿名函数的特性， 而匿名函数则是在需要一个函数，但是又不想费力去命名一个函数的情况下去使用的。这样的场景其实有很多很多， 所以匿名函数几乎是现代编程语言的标配。

#### 左值右值

* **左值右值，右值引用，为什么引入右值引用**

  * 左值是具有可寻址的存储单元，并且能由用户改变其值的量，比如，一个int、float、class对象等。左值具有持久的状态，离开作用于后才会销毁。
  * 右值表示即将销毁的临时对象，具有短暂的状态如字面值常量“hello”，返回非引用类型的表达式int func()等，都会生成右值；表达式结束后就不再存在的临时对象。
  * 右值引用必须绑定到右值的引用，通过&&两个取地址符号获得右值引用，只能绑定到即将销毁的对象，可以自由移动其资源。
  * 右值引用是为了支持移动操作而引出的一个概念，它只能绑定到一个将要毁灭的对象，使用右值引用可以避免无谓的拷贝，提高性能。使用std::move可以将左值转换为右值引用。

  

* **为什么要自己定义构造函数？深拷贝与浅拷贝**

  * 拷贝构造函数的作用就是定义了当我们用同类型的另一个对象初始化本对象的时候发生的行为，在某些情况下，使用默认的会出错。比如一个类里面有指针，使用默认的是只拷贝指针的值，即两个对象指向同一块内存，那么其中一个对象析构以后，这一块内存可能不复存在，另一个指针就变为了悬浮指针。
  * 这就是深拷贝和浅拷贝的区别，浅拷贝只是简单地复制某个对象的指针，新旧对象共享同一块内存；深拷贝会另外创造一片新的内存，新对象与老对象不会共享内存，修改新对象不会改到原对象。

  

* **什么是移动构造函数，和拷贝构造函数有什么区别**

  ​		移动构造函数需要传递一个右值引用，不会分配新的内存，而是接管传递而来对象的内存，并在移动之后把源对象销毁；拷贝构造传的是一个左值引用，可能会造成新的内存分配，效率比较低。

  

* **移动语义与完美转发**

传统 C++ 通过拷贝构造函数和赋值操作符为类对象设计了拷贝/复制的概念，但为了实现对资源的移动操作， 调用者必须使用先复制、再析构的方式，浪费时间。

直接通过右值引用，接管将亡值的资源管理，延长生命周期。

完美转发:

```cpp
#include <iostream>
#include <utility>
void reference(int& v) {
    std::cout << "左值引用" << std::endl;
}
void reference(int&& v) {
    std::cout << "右值引用" << std::endl;
}
template <typename T>
void pass(T&& v) {
    std::cout << "              普通传参: ";
    reference(v);
    std::cout << "       std::move 传参: ";
    reference(std::move(v));
    std::cout << "    std::forward 传参: ";
    reference(std::forward<T>(v));
    std::cout << "static_cast<T&&> 传参: ";
    reference(static_cast<T&&>(v));
}
int main() {
    std::cout << "传递右值:" << std::endl;
    pass(1);

    std::cout << "传递左值:" << std::endl;
    int v = 1;
    pass(v);

    return 0;
}
```

### 智能指针与内存管理

智能指针都遵循RAII原理，RAII是对资源申请、释放的操作的一种封装。

#### shared_ptr

* 是一种强引用智能指针，能够记录多少个`shadred_ptr`共同指向一个对象，当引用计数为0的时候对象会自动删除。

```cpp
template<typename T>
class smart
{
private:
	T* _ptr;
	int* _count; //reference couting

public:
	//构造函数
	smart(T* ptr = nullptr) :_ptr(ptr)
	{
		if (_ptr)
		{
			_count = new int(1);
		}
		else
		{
			_count = new int(0);
		}
	}

	//拷贝构造
	smart(const smart& ptr)
	{
		if (this != &ptr)
		{
			this->_ptr = ptr._ptr;
			this->_count = ptr._count;

			(*this->_count)++;
		}
	}

	//重载operator=
	smart& operator=(const smart & ptr)
	{
		if (this->_ptr == ptr._ptr)
		{
			return *this;
		}
		if (this->_ptr)
		{
			(*this->_count)--;
			if (*this->_count == 0)
			{
				delete this->_ptr;
				delete this->_count;
			}
		}
		this->_ptr = ptr._ptr;
		this->_count = ptr._count;
		(*this->_count)++;
		return *this;
	}

	//operator*重载
	T& operator*()
	{
		if (this->_ptr)
		{
			return *(this->_ptr);
		}
	}

	//operator->重载
	T* operator->()
	{
		if (this->_ptr)
		{
			return this->_ptr;
		}
	}

	//析构函数
	~smart()
	{
		(*this->_count)--;
		if (*this->_count == 0)
		{
			delete this->_ptr;
			delete this->_count;
		}
	}
	//return reference couting
	int use_count()
	{
		return *this->_count;
	}
};
```

#### unique_ptr

* 这是一种独占的智能指针，禁止与其他智能指针共享同一个对象。

```cpp
template<typename T>
class UniquePtr
{
public:
	UniquePtr(T *pResource = NULL)
		: m_pResource(pResource)
	{

	}

	~UniquePtr()
	{
		del();
	}

public:
	void reset(T* pResource) // 先释放资源(如果持有), 再持有资源
	{
		del();
		m_pResource = pResource;
	}

	T* release() // 返回资源，资源的释放由调用方处理
	{
		T* pTemp = m_pResource;
		m_pResource = nullptr;
		return pTemp;
	}

	T* get() // 获取资源，调用方应该只使用不释放，否则会两次delete资源
	{
		return m_pResource;
	}

public:
	operator bool() const // 是否持有资源
	{
		return m_pResource != nullptr;
	}

	T& operator * ()
	{
		return *m_pResource;
	}

	T* operator -> ()
	{
		return m_pResource;
	}

private:
	void del()
	{
		if (nullptr == m_pResource) return;
		delete m_pResource;
		m_pResource = nullptr;
	}

private:
	UniquePtr(const UniquePtr &) = delete; // 禁用拷贝构造
	UniquePtr& operator = (const UniquePtr &) = delete; // 禁用拷贝赋值

private:
	T *m_pResource;
};
```

#### weak_ptr

* 是一种弱引用智能指针，不会引起计数的增加，进而解决循环引用问题。

### 多线程

* **thread**



## 关键字

### const

* const用来修饰定义常量，具有不可变性。在类中，被const修饰的成员函数，不能修改类中的数据成员，加mutable关键字后可以修改。
* 指针常量指的是指针是一个常量，不能被修改，但是指针指向的对象可以被修改，常量指针指的是这个指针指向的对象是一个常量，指针本身可以被修改。
* const修饰的函数可以重载。const成员函数既不能改变类内的数据成员，也不能调用非const的成员函数；const对象只能调用const函数，非const对象无论是否为const成员函数都可以调用，但是如果有重载的非const函数，会优先调用非const函数。
* 顶层const：本身是const；底层const：指向的对象是一个const

### static

* static作用：控制变量的存储方式和可见性。
* 作用
  * 修饰局部变量，会将数据放到静态数据区(原本在栈区)，生命周期会延续到程序结束，作用域并不改变。
  * 修饰全局变量，修改了可见性，只有本文件可见
  * 修饰函数，也是改变了作用域
  * 修饰类中的函数与变量，表示由所有对象所有，存储空间只存在一个副本，静态非常量数据成员，只能在类外定义和初始化，在类中只是声明。

### explicit

表明类的构造函数是显式的，不能隐式转换。

### constexpr

告诉编译期，应该是一个常量，方便编译器优化

### volatile

禁止编译器优化

### mutable

可变的意思，使类中被声明为const的函数可以修改类中的非静态成员。

## 内联函数和宏

* **内联函数的作用与缺点**
  * 作用：在编译的时候，将调用内联的地方直接将内联函数的代码块替换，节省了函数调用带来的开销。
  * 缺点：可能会造成代码的膨胀，编译时间比较缓慢，造成比较大内存开销，exe比较大，占用CPU资源

* **内联和宏的区别**
  * define是在预处理阶段对命令进行替换，inline是在编译的时候，将调用内联的地方直接将内联函数的代码块替换，节省了函数调用带来的开销。
  
  * define不会对参数的类型进行检查，会出现类型安全的问题；但是内联函数在编译阶段会进行类型检查；
  
  * 宏定义时要注意书写（参数要括起来）否则容易出现歧义，内联函数不会产生歧义；
  





## STL

* **STL六大组件和关系**

  容器、迭代器、算法、适配器、空间分配器、仿函式

  ![image-20231206085740904](C++.assets/image-20231206085740904.png)

  * 容器：各种数据结构，vector、list、deque、set、map
  * 算法：各种常用算法，包含了初始化、排序、搜索、转换等等
  * 迭代器：用于遍历对象集合的元素，扮演着容器和算法之间的胶合剂，可以视为“泛型指针”。
  * 仿函数：也成为函数对象，行为类似函数，可以作为算法的某种策略。
  * 适配器：一种用来修饰容器或者仿函数活迭代器接口的东西。例如STL提供的queue、stack，就是一种空间配接器。
  * 分配器：空间支配器，负责空间的管理与配置。


### STL各种容器的底层实现

#### **vector**

* 底层是一块具有连续内存的数组，vector核心就在于其长度自动可变，vector的数据结构由三个迭代器实现：指向首元素的start，指向尾元素finish，指向内存末端的end_of_storage。
* 扩容机制：当目前可用的空间不足时，分配目前空间的两倍或者目前空间加上所需的新空间大小，容量的扩张必须经过“重新分配内存、元素拷贝、释放原空间”等。

#### **list**

list底层是一个循环双向链表。

#### **deque**

​		双向队列，**通过建立 map 数组，deque 容器申请的这些分段的连续空间就能实现“整体连续”的效果**。

![image-20231206113445969](C++.assets/image-20231206113445969.png)

#### **stack和queue**

栈和队列。基于deque实现，属于容器配接器。

#### **priority_queue**

优先队列，底层为vector容器，以heap作为处理规则，heap是一个完全二叉树。

#### **set和map**

底层都是红黑树实现，红黑树是一种二叉搜索树，平衡二叉树(AVL)和红黑树的区别：AVL 树是高度平衡的，频繁的插入和删除，会引起频繁的rebalance（旋转操作），导致效率下降；红黑树不是高度平衡的，算是一种折中，插入最多两次旋转，删除最多三次旋转。



* **STL内存管理方式，Allocator次级分配器的原理以及内存池的优势和劣势**
  * 为了提升内存管理的效率，减少申请小内存导致的内存碎片的问题，STL采用了两级配置器，当分配空间大小超过128B的时候，会使用第一级空间配置器，直接使用malloc、realloc、free等函数进行内存空间的分配和释放，如果空间分配大小小于128B，将使用第二级空间配置器，采用了内存池技术，通过空闲链表来管理内存。
  * 次级配置器的内存池管理技术：每次配置一大块内存，并维护对应的自由链表(free list)。若下次再有相同大小的内存配置，就直接从自由链表中拔出。如果客户端释还小额区块，就由配置器回收到自由链表中；配置器共要维护16个自由链表，存放在一个数组里，分别管理大小为8-128B不等的内存块。分配空间的时候，首先根据所需空间的大小（调整为8B的倍数）找到对应的自由链表中相应大小的链表，并从链表中拔出第一个可用的区块；回收的时候也是一样的步骤，先找到对应的自由链表，并插到第一个区块的位置。
  * 优势：避免了内存碎片(外部碎片)，不需要频繁从用户态切换到内核态
  * 劣势：仍会造成一定的内存浪费、比如申请120B就必须分配128B（内部碎片）。
* **STL容器的push_back和emplace_back的区别？**
  - 传统C++中，emplace/emplace_back函数使用传递来的参数直接在容器管理的内存空间中构造元素（只调用了构造函数）；push_back会创建一个局部临时对象，并将其压入容器中（可能调用拷贝构造函数或移动构造函数）
  - 现代C++无区别

* **STL的排序用到了哪种算法，具体如何执行？**
  * 快速排序、插入排序和堆排序；当数据量很大的时候用快排，划分区段比较小的时候用插入排序，当划分有导致最坏情况的倾向的时候使用堆排序。
* **排序算法分析**

![image-20231206114912470](C++.assets/image-20231206114912470.png)

* **哈希表**
  * 哈希散列可能会存在冲突，解决冲突一般有开放定址法法和拉链法，开放定址法包括线性测探、平方测探法，本质上都是对应位置被占用了就向后查找；
  * 哈希表的长度使用质数，可以降低发生冲突的概率，使哈希后的数据更加均匀，如果使用合数，可能会导致很多数据集中分布到一个点上，造成冲突；

### STL容器算法性能比较





## 工程问题

* **如果一个类有其他的类作为数据成员，构造函数调用顺序是怎么样的？**

成员对象的构造函数是在主题构造函数体开始之前调用的，基类的构造函数会在派生类构造函数之前调用。

### 编译原理

* **编译链接原理，源文件->可执行文件**

  预处理、编译、汇编、链接

  * 预处理阶段处理头文件包含关系，宏定义，对预编译命令进行替换，生成预编译文件；
  * 编译阶段将预编译文件编译，生成汇编文件（编译的过程就是把预处理完的文件进行一系列的词法分析，语法分析，语义分析及优化后生成相应的汇编代码)；
  * 汇编阶段将汇编文件转换成机器码，生成可重定位目标文件（.obj文件）（汇编器是将汇编代码转变成机器可以执行的命令，每一个汇编语句几乎都对应一条机器指令。汇编相对于编译过程比较简单，根据汇编指令和机器指令的对照表一一翻译即可
  * 链接阶段，将多个目标文件和所需要的库连接成可执行文件（.exe文件）



![image-20231206115945016](C++.assets/image-20231206115945016.png)

* **静态库与动态库**

  * 静态库

  静态库链接是在编译器完成，浪费空间和资源，在链接阶段，会与汇编生成的obj文件一起链接生成可执行文件。

  * 动态库

  在程序编译的时候不会链接到目标代码，是在程序运行时才被载入的，因此链接时动态链接。将一些程序升级变得简单。解决了静态库对程序的更新、部署和发布页会带来麻烦。用户只需要更新动态库即可，**增量更新**。


* **如何优化内存**
  * 减少内存泄露。避免使用不必要的局部变量和静态变量，及时释放不再使用的内存空间。
  * 优化内存分配。合理分配内存，避免频繁的申请和释放。
  * 避免内存碎片。使用内存池技术，复用分配好的内存。



### 对象池思想

对于频繁创建和销毁的对象，对象池的思想是，首先从给对象池中寻找有没有可用的对象，如果没有就创建对象来使用，然后当一个对象不使用的时候，不是把它删除，而是将它设置为未激活状态并放在对象池中，等需要使用的时候再去对象池中寻找，并把它激活。





# 设计模式

## 单例模式

保证一个类仅有一个实例，并提供一个访问它的全局访问点，该实例被所有程序模块共享。

* **懒汉单例**

单例实例在第一次被使用时才进行初始化，这叫做延迟初始化。线程不安全，需要加锁，并且不能简单的在前面判空加锁，因为可能某个线程正在初始化单例，另一个线程却在判断单例是否为空，这样就会获取到不完全的单例，造成错误。

* **饿汉单例**

在main函数开始的时候即创建对象，线程安全；

## 工厂模式

- 该模式用来封装和管理类的创建，终极目的是为了解耦，实现创建者和调用者的分离。
- 简单工厂

![image-20231206202155772](C++.assets/image-20231206202155772.png)



* 工厂方法

![image-20231206202215318](C++.assets/image-20231206202215318.png)

* 抽象工厂

![image-20231206202426779](C++.assets/image-20231206202426779.png)

## 观察者模式

* 又叫发布-订阅模式（Publish/Subscribe），定义对象间一种一对多的依赖关系，使得每当一个对象改变状态，则所有依赖于它的对象都会得到通知并自动更新。该模式属于行为型模式。

  ![image-20231206202555636](C++.assets/image-20231206202555636.png)

# 图形学

## 渲染管线

* **什么是图形渲染管线，分为哪些阶段？**

图形渲染管线实际上指的是一堆原始图形数据途径一个输送管道，期间经过各种处理变化最终出现到屏幕的过程，在概念上可以分为四个阶段：应用程序阶段、几何阶段、光栅化阶段、像素处理阶段。

（1）应用程序阶段，该阶段主要是在软件层面上执行的一些工作，包括空间加速算法、视锥剔除、碰撞检测、动画物理模拟等。大体逻辑是：执行视锥剔除，查询出可能需要绘制的图元并生成渲染数据，设置渲染状态和绑定各种Shader参数，调用DrawCall，进入到下一个阶段，GPU渲染管线。

（注：应用程序阶段在**CPU**端完成，后面的所有阶段都是在**GPU**端完成）

（2）几何阶段，包含顶点着色、投影变换、裁剪和屏幕映射阶段。

a. 顶点处理阶段：这个阶段会执行**顶点变换**和**顶点着色**的工作。通过模型矩阵、观察矩阵和投影矩阵(也就是MVP矩阵)计算出顶点在裁剪空间下的位置(clip space)，以便后续阶段转化为标准化设备坐标系(NDC)下的位置。也可能会计算出顶点的法线(需要有法线变换矩阵)和纹理坐标等。同时，在这个阶段也可能会进行顶点的着色计算，如平面着色 (Flat Shading)和高洛德着色 (Gouraud Shading)都是在顶点着色器中进行着色计算。因为这个阶段是完全可控制的，因此执行什么样的操作由程序员来决定。（此外，在顶点处理阶段的末尾，还有一些可选的阶段，包括曲面细分(tessellation)、几何着色(geometry shading)和流输出(stream output)，此处不详细描述）

b. 裁剪阶段：对部分不在视体内部的图元进行裁剪。这部分是几乎完全由硬件控制的，因此没必要详细描述，至于为什么有了视锥剔除，到这个阶段还需要进行一次裁剪，可参考这个问题[为什么在ndc归一化坐标已经包含了视锥体剔除功能的情况下 还需要视锥体裁剪？ - 知乎 (zhihu.com)](https://www.zhihu.com/question/304277310/answer/562221670)。简单来说就是两次裁剪的粒度不同，前者是在物体对象层面的，一般对对象的包围盒做剔除，剔除掉不在视锥体内的物体，NDC裁剪是在三角形层面做的，裁剪掉不在屏幕内的像素。

![image-20231211220507817](C++.assets\image-20231211220507817.png)

c. 屏幕映射阶段：主要目的是将之前步骤得到的坐标映射到对应的屏幕坐标系上。

（3）光栅化阶段，包含三角形设置和三角形遍历阶段。

![image-20231211220538518](C++.assets\image-20231211220538518.png)

a. 三角形设置(图元装配)，计算出三角形的一些重要数据(如三条边的方程、深度值等)以供三角形遍历阶段使用，这些数据同样可用于各种着色数据的插值。

b. 三角形遍历，找到哪些像素被三角形所覆盖，并对这些像素的属性值进行插值。通过判断像素的中心采样点是否被三角形覆盖来决定该像素是否要生成片段。通过三角形三个顶点的属性数据，插值得到每个像素的属性值。此外透视校正插值也在这个阶段执行。



（4）**像素处理阶段**，包括像素着色和测试合并。

a. 像素着色，进行光照计算和阴影处理，决定屏幕像素的最终颜色。各种复杂的着色模型、光照计算都是在这个阶段完成。

b. 测试合并，包括各种测试和混合操作，如裁剪测试、透明测试、模板测试、深度测试以及色彩混合等。经过了测试合并阶段，并存到帧缓冲的像素值，才是最终呈现在屏幕上的图像。

（5）**各个阶段的可控性**

![image-20231211220654478](C++.assets\image-20231211220654478.png)



* **GPU渲染管线有哪些流程**

**应用程序-》顶点着色器-》曲面细分着色器-》几何着色器-》裁剪与消除-》屏幕映射-》光栅化-》像素着色-》深度模板测试与混合**



* **简述OpenGL中由顶点数据输入会知道一幅图像的具体过程**

  (1) vbo将数据存储到缓存中，vao绑定顶点属性关系，然后vbo将缓存数据传给vertex_shader；

  (2) 在顶点着色器中进行坐标变换，由mvp矩阵将其转换到裁剪坐标系，以及顶点着色；

  (3) 然后到了图元装配阶段，将顶点着色器的输出数据装配成指定图元的形状，之后还有一个可选的几何着色器阶段，将输入的点或线扩展成多边形；

  （注意，这个地方的表述正是和平常的图形渲染管线不一致的地方，这里应该是将图形渲染管线中的三角形设定或者说图元组装阶段表述为图元装配阶段，然后下面的光栅化阶段就是三角形遍历阶段）

  (4) 然后到裁剪和屏幕映射阶段；裁剪掉视体外的图元，将当前坐标映射到屏幕坐标；

  (5) 然后进入光栅化阶段，找到哪些像素被三角形覆盖，以及进行插值计算；

  (6) 然后进入到了fragment_shader，执行光照计算，进行着色；

  (7) 最后进入到测试混合阶段，包括Alpha测试、模板测试、深度测试等，然后进行混合。



* **什么是early-Z？什么时候会失效？**

early-Z就是提前进行深度测试，剔除不可见的片段，以提高渲染性能。

失效情况：

	1. 开启了alpha test。early-Z是测试完就会更新深度缓存，如果某个片元通过了测试，但是后面alpha test没通过，那这块像素被丢弃，假如该位置上后面的片元都无法通过测试，那就直接G了。
	1. 手动修改GPU插值得到的深度
	1. 关闭了深度测试



* **什么时候vs比fs运行的次数还要多**
  * 提前模板测试(?)
  * 自定义裁剪空间(?)



* **除了 fs、vs 还有哪些着色器？它们的作用？写代码时用过哪些着色器？**

1. 几何着色器，用于处理几何图形，通常在顶点着色器喝片元着色器之间执行，用于生成新的几何图形、增加几何细节、执行投影和裁等等
2. 细分着色器是用于曲面细分的一对着色器。TCS（细分控制着色器）用于控制如何细分曲面，而TES（细分评估着色器）用于计算细分曲面的最终顶点坐标。
3. 计算着色器，可以执行通用计算任务，而不涉及图像渲染，通常用于GPGPU任务，数据处理以及物理模拟等等。



* **GPU与CPU的区别**

  * CPU存在性能限制，而GPU不存在，我们可以吃掉GPU的所有运算资源，而不能吃掉所有CPU的运算资源，否则会导致系统崩溃
  * CPU性能再高，是以高并发的形式调度的，尽管现在有4核、8核，但始终不够自由，而GPU实现了真正的并行，GPU有很多小的运算单元，可以同时完成简单的运算。
    * 如果一个计算内核做的很小，但是做很多个，那么我们就可以同时在这些核上做同样的运算，基于SIMD模型，这就是GPU算力强的原因。

* **什么是DrawCall**

  * 在应用阶段，尽管CPU把数据准备得十分充分，在完成传送任务后，CPU也不能一走了之，还需要向GPU下达一个渲染命令，这个命令就是Draw Call，由于之前我们把这个数据准备得十分完善了，所以DrawCall仅仅是一个指向被渲染的图元列表，没有其他材质信息。
  * CPU向GPU发送指令也是像流水线一样，CPU王命令缓冲区中一个个放入命令，GPU一个个取出，在实际的渲染中，GPU的渲染速度往往超过了CPU的提交命令的速度，这就导致大部分时间都消耗在了CPU的Draw Call上，有一种解决办法是**批处理**，即要把渲染的模型合并在一起交给GPU。

  ![img](https://cdn.nlark.com/yuque/0/2023/webp/29680306/1689081816366-4963a7b0-8f8d-44d2-9bcc-5df9e5be5773.webp?x-oss-process=image%2Fresize%2Cw_498%2Climit_0)

* **各种测试的含义以及相对顺序**

  * 裁剪测试，在裁剪测试中，允许程序员开设一个裁剪框，只有在裁剪框内的片元才会被显示出来，在外部的偏远均被剔除，通常情况下，我们会让视口的大小和屏幕空间一样大，此时可以不需要使用到裁切测试。但当两者大小不一样大时，我们需要用到裁切测试来避免其产生的一些问题。如下图所示。

  ![img](https://cdn.nlark.com/yuque/0/2023/jpeg/29680306/1689165224077-10845338-4888-42f1-b945-b509513cc905.jpeg?x-oss-process=image%2Fresize%2Cw_554%2Climit_0%2Finterlace%2C1)

  * Alpha测试：像素值一般是由RGBA四个分量来表示的，其中的A是alpha，表示的是物体的不透明度。1代表完全不透明，0代表完全透明。可选的 alpha 测试可在深度测试执行前在传入片段上运行。片段的 alpha 值与参考值作某些特定的测试（如等于，大于等），如果片段未能通过测试，它将不再进行进一步的处理。 alpha 测试经常用于不影响深度缓存的全透明片段的处理。简单来说，就是根据物体的透明度来决定是否渲染。
  
  * 模板测试：模板缓冲是用于记录所呈现图元位置的离屏缓存，如下图所示，如果使用了模板缓冲，就相当于在屏幕上有一块模板盖在上面，只有位于这个模板中的图元片段，才会被渲染出来。模板测试就是用片段指定的参考值与模板缓冲中的模板值进行比较，如果达到预设的比较结果，模板测试就通过了，然后用这个参考值更新模板缓冲中的模板值；如果没有达到预设的比较结果，就是没有通过测试，就不更新模板缓冲。简单来说，就是根据物体的位置范围决定是否渲染。
  
    
  
    ![img](https://cdn.nlark.com/yuque/0/2023/jpeg/29680306/1689165224081-42430013-3268-453b-9131-90a317810a1e.jpeg)
  
  * 深度测试： 我们在观察物体的时候，位于前面的物体会把后面的物体挡住，所以在渲染的时候，图形管线会先对每一个位置的像素存储一个深度值，称为深度缓冲，代表了该像素点在3D世界中离相机最近物体的深度值。于是在计算每一个物体的像素值的时候，都会将它的深度值和缓冲器当中的深度值进行比较，如果这个深度值小于缓冲器中的深度值，就更新深度缓冲和颜色缓冲的值，否则就丢弃（深度测试和深度写入）；简单来说，就是根据物体的深度决定是否渲染。
  
  * 测试顺序:裁剪测试-》alpha测试-》模板测试-》深度测试
  
    
  



* **延迟渲染的优缺是什么？在移动端需要考虑什么问题**

优点：

	1. 支持大量光源。延迟渲染可以支持大量官员，他仅需要对场景的几何信息进行一次渲染，然后为每个光源执行光照计算，而不必对每个光源进行完整的渲染。
	1. 灵活性，延迟渲染允许多个光照和材质通道，以实现复杂的光照效果。
	1. 后期处理，延迟渲染可以方便的进行后期处理，如SSAO等。

缺点：

1. gbuffer显存占用比较大，存储的信息比较多，几何、法线、深度、材质等等。
2. 透明物体问题，延迟渲染不适用于大量半透明物体，因为它难以正确处理半透明物体的混合效果。
3. 硬件要求，需要叫高性能的GPU

在移动端，主要需要考虑：

1. **性能考虑**：移动设备的GPU性能有限，因此需要仔细优化渲染管线，以确保渲染保持流畅。
2. **内存管理**：移动设备的内存有限，需要小心管理渲染缓冲区，以避免内存泄漏和性能问题。
3. **适应硬件**：不同移动设备的GPU性能和特性不同，需要为不同的设备进行适配。
4. **移动设备特性**：移动设备通常具有触摸屏、加速计、陀螺仪等特性，可以在游戏中进行互动和控制，需要考虑如何集成这些特性。
5. **电池寿命**：在移动设备上，渲染过程对电池寿命有影响，需要优化以减少能耗。

在移动端可以将多个 gbuffer 压缩成一张纹理，苹果的 metal 有实现移动端 gbuffer 的优化，通过 Lossy compression 压缩 Render Target 的大小以节省显存带宽



* **Gamma 校正，何时切换到gamma空间**

显示器电压和亮度的变换不是线性的，而是 gamma 2.2

在线性空间计算光照、颜色，输出的时候要做一次 gamma 0.454 抵消显示器的变换



* **顶端属性太多，槽位不够怎么办**

1. 可以用一个大Buffer存struct，然后顶点属性只存索引，类似于opengl生成一个大的VBO，再有一个VAO根据偏移设定不同的顶点属性。



* **vs、fs开销很大，怎么优化解决？**

vs：避免连续矩阵乘法，可以在 cpu 上预先算好；vs 开销大也可能是远处高模几何体太多，LOD简化

fs：检查 over draw 现象是否严重。检查代码有无分支、大循环。检查某些 feature 能否降分辨率实现



* **CSM问题**

draw call 开销大，显存带宽开销大

## 辐射度量学

### 基础名词

* 辐射能(Radiant energy)Q：电磁辐射的能量，单位是$J$

![image-20231210103842393](C++.assets\image-20231210103842393.png)

* 辐射通量(Radiant flux)或功率(power)$\phi$:单位时间释放、反射、投射、或者接受的能量。单位是$W$或者$lm$

![image-20231210104030062](C++.assets\image-20231210104030062.png)

* 辐射强度(Radiant Intensity):辐射强度是单位立体角(solid angle)由点光源发出的功率（power）。

![image-20231210104854620](C++.assets\image-20231210104854620.png)

各向同性点源：

![image-20231210105007878](C++.assets\image-20231210105007878.png)

* 辐照度(Irradiance)

辐照度就是每(垂直投影)单位面积入射到一个表面上一点的辐射通量。

![image-20231210105442827](C++.assets\image-20231210105442827.png)

兰伯特余弦定律：**表面辐照度**与**光方向和表面法线夹角的余弦值**成**正比**(也就是说只要在表面法线方向的的辐射度分量)。

![image-20231210105502510](C++.assets\image-20231210105502510.png)



* 辐射(Radiance)：是指一个表面在**每单位立体角、每单位投影面积**上所发射(emitted)、反射(reflected)、透射(transmitted)或接收(received)的**辐射通量(功率)**。

![image-20231210105541112](C++.assets\image-20231210105541112.png)

**辐射强度(Radiant Intensity)、辐照度(Irradiance)、辐射(Radiance)三者关系：**

辐射强度：单位立体角的辐射通量

辐照度：单位投影面积的辐射通量

辐射：**单位投影面积**的**辐射强度**或者是**单位立体角**的**辐照度**(单位立体角、单位投影面积的辐射通量)

**入射辐射**(Incident Radiance)：指**到达表面的单位立体角**的**辐照度**。即它是沿着给定光线到达表面的光(入射方向指向表面)

![image-20231210105622965](C++.assets\image-20231210105622965.png)

**出射辐射**(Exiting Radiance)：**离开表面**的**单位投影面积**的**辐射强度**。例如：对于面光(area light)，它是沿着给定光线发射的光(出射方向指向表面)

![image-20231210105640778](C++.assets\image-20231210105640778.png)

### **辐照度**(Irradiance) **VS. 辐射**(Radiance)

辐照s度：在面积$dA$ 的总辐射通量

辐射： 在面积$dA$ 、方向$dw$ 上的辐射通量

![image-20231210105746031](C++.assets\image-20231210105746031.png)

### BRDF

* **基本概念**

双向反射分布函数(Bidirectional Reflectance Distribution Function)，是一个用来描述物体表面如何反射光线的方程，表示了给定一条入射光的时候，某一条特定的出射光线的性质是怎么样的。定义的是出射辐射率(Radiance)的微分和入射光辐射度(Irradiance)的微分之比。

![image-20231212150906539](C++.assets/image-20231212150906539.png)

入射光找到物体表面上，反射光线为v，那么反射光的亮度(辐射率)和入射光的能量(辐照度)会成一个比例，这个比例就是BRDF，可以理解为，在某一个特定角度观看某个点时，各个方向的入射光对该点的最终光亮度产生的贡献比例。

![image-20231212151320385](C++.assets/image-20231212151320385.png)

![image-20231212151334076](C++.assets/image-20231212151334076.png)





#### cook-torrance模型

一般使用一种被称为Cook-Torrance BRDF的模型。

1. **整体公式**

![image-20231212154544849](C++.assets/image-20231212154544849.png)

2. **镜面反射部分**

![image-20231212154939627](C++.assets/image-20231212154939627.png)

- **法线分布函数**：估算在受到表面粗糙度的影响下，朝向方向与半程向量一致的微平面的数量。这是用来估算微平面的主要函数。
- **几何函数**：描述了微平面自成阴影的属性。当一个平面相对比较粗糙的时候，平面表面上的微平面有可能挡住其他的微平面从而减少表面所反射的光线。
- **菲涅尔方程**：菲涅尔方程描述的是在不同的表面角下表面所反射的光线所占的比率。

分母是校正因子，作为微观几何局部空间和宏观几何局部空间变换的校正。

### 渲染方程

* **基本概念**

渲染方程是一个描述光能在场景中流转的方程，基于能量守恒定律，在理论上给出了一个完美的光能求解结果。

含义：在某个视点看向特定的位置x，看到的出射光亮度(辐照率)Lo等于x点自发光亮度Le(辐射率)以及该点的反射光亮度之和，可以由以下公式表示：

![image-20231212151626025](C++.assets/image-20231212151626025.png)

![image-20231212151641104](C++.assets/image-20231212151641104.png)



反射方程：其实就是除去了自发光的部分，只保留半球积分部分。





### PBR

* **什么是PBR**

  基于物理的渲染(Physically Based Rendering， PBR)指的是基于物理原理和微平面理论建模的着色/光照模型，使得渲染效果更加真实。常见基础理念：

  * **微平面理论：**微平面理论是将物体表面建模成做无数微观尺度上有随机朝向的理想镜面反射的小平面（microfacet）的理论。在实际的PBR 工作流中，这种物体表面的不规则性用粗糙度贴图或者高光度贴图来表示。
  * **能量守恒：**出射光能量永远不可能超过入射光能量。随着粗糙度的上升，镜面反射区域会增大，同时反射区域的平均亮度会减少。
  * **菲涅尔反射：**光线从不同角度入射有不同的反射率。相同的入射角度，不同的物质也会有不同的反射率。F0是即0度角入射的菲涅尔反射值。大多数非金属的F0范围是0.02~0.04，大多数金属的F0范围是0.7~1.0。
  * **线性空间：**光照计算必须要在线性空间完成。shader中输入的gamma空间的贴图比如漫反射贴图需要被转换成线性空间，在具体操作时需要根据不同引擎和渲染器的不同做不同的操作；而描述物体表面属性的贴图，粗糙度、高光贴图、金属贴图等必须是线性空间。
  * **色调映射：**是将宽范围的照明级别拟合到屏幕有限色域内的过程，因为基于HDR渲染出来的亮度可能会超过显示器的最大亮度，所以需要色调映射，将光照结果从HDR转换到LDR。
  * **物体的光学特性：**现实世界中有不同类型的物质可分为三大类：绝缘体（Insulators），半导体（semi-conductors）和导体（conductors）。在渲染和游戏领域，我们一般只对其中的两个感兴趣：导体（金属）和绝缘体（电解质，非金属）。其中非金属具有单色/灰色镜面反射颜色。而金属具有彩色的镜面反射颜色。即非金属的F0是一个float。而金属的F0是一个float3，如下图。
    ![image-20231212153133884](C++.assets/image-20231212153133884.png)

* **PBR的范畴**



![image-20231212153554834](C++.assets/image-20231212153554834.png)

#### **PBR Material**

#### **PBR Lighting**

PBR的光照可以通过反射方程来计算，分为两步，一部分是漫反射，一部分是镜面反射，如下图的公式所示：

![image-20231212155407631](C++.assets/image-20231212155407631.png)

![image-20231212155322680](C++.assets/image-20231212155322680.png)

* 漫反射部分的$f_{lambert}$是一个常数，直接积分就可以了。通过环境立方贴图来获取每一个方向的radiance，然后对于每一个出射方向的积分结果存储到一张辐照度贴图中(对radiance)的卷积，在实时渲染中直接采样出射方向就可以得到积分结果。

![image-20231212163246436](C++.assets/image-20231212163246436.png)



根据离散公式，均匀采样求结果平均值，公式转化为离散版本：

![image-20231212163452213](C++.assets/image-20231212163452213.png)



* 镜面反射部分比较麻烦，先用分割近似求和的方法，将积分划分为两个卷积式子：

![image-20231212160351576](C++.assets/image-20231212160351576.png)

第一部分是预滤波环境贴图，将不同粗糙度的卷积结果存到不同的mipmap中(不同粗糙度对着不同的mipmap级别，处于中间的粗糙度可以插值)。同时还做了假设镜面反射方向——总是等于输出采样方向$w_0$。

![image-20231212163802457](C++.assets/image-20231212163802457.png)

所有可能出射的反射光构成的形状称为镜面波瓣。随着粗糙度的增加，镜面波瓣的大小增加；随着入射光方向不同，形状会发生变化。因此，镜面波瓣的形状高度依赖于材质。 在微表面模型里给定入射光方向，则镜面波瓣指向微平面的半向量的反射方向。考虑到大多数光线最终会反射到一个基于半向量的镜面波瓣内，采样时以类似的方式选取采样向量是有意义的，因为大部分其余的向量都被浪费掉了，这个过程称为重要性采样。



第二部分是BRDF部分，他可以进一步拆分为跟菲涅尔项相关的两部分，分别代表菲涅尔相应的比例和偏差，然后对这两项做卷积预运算，并存储到一张查找贴图中(look-up texture, LUT)。渲染时可以将n·wi作为横坐标，以粗糙度roughness作为纵坐标，去LUT中采样获得该条件下的BRDF响应结果，以加快计算速度。

![image-20231212164948931](C++.assets/image-20231212164948931.png)

![image-20231212165005134](C++.assets/image-20231212165005134.png)



![image-20231212165301142](C++.assets/image-20231212165301142.png)



#### **PBR Camera**

* **谈一下BRDF中，D、F、G项？菲涅尔项会带来什么样的视觉效果？**

1. D：表示的是微表面结构中法线分布函数，它描述了光线以多大的概率在不同方向上的散射。业界常用的法线分布函数是GGX，具有更好的高光长尾

   ![image-20231212153920593](C++.assets/image-20231212153920593.png)

2. F：菲涅尔项，表示的是光线在材质表面与介质之间的反射和折射的行为，掠视金属时反射较多的光而俯视时反射光较少

   ![image-20231212154008402](C++.assets/image-20231212154008402.png)

3. G：表示的是几何遮蔽的情况，返回一个未被遮蔽的表面的百分比，常用GGX模型，通过史密斯法叠加入射和出射两个方向。







* **PBR 材质贴图很多，纹理槽位不够应该怎么处理？**

1. 合并多个属性到一个通道，比如将roughness和metallic可以存在8bit纹理的高低4bit上
2. 虚拟纹理，将小贴图合并成大贴图，按需调入



* **PBR贴图格式需要注意什么？**

1. **颜色空间**：确保在加载贴图时使用正确的颜色空间。漫反射和环境反射贴图通常使用sRGB颜色空间，而法线贴图和金属度贴图通常使用线性颜色空间。

## 三维变换

* **法线变换矩阵和模型变换矩阵的关系**

  * 法线矩阵

  有很多计算工作是在 **观测空间 ( eye space )** 下完成的，其中包括与光照相关的计算。如果不在观测空间计算，与观测位置相关的效果将很难实现，如高光 ( specular )。

  因此，我们需要一种方法，将法线转换到观测空间。将顶点变换到观测空间的计算，可以写成：

  ```cpp
  vertexEyeSpace = gl_ModelViewMatrix * gl_Vertex;
  ```

  那为什么不能对法线做一遍同样的操作呢？法线是有 3 个浮点数分量的向量，*模型-观测矩阵* 是 4x4 的矩阵。法线是一个向量，我们只想改变其方向。*模型-观测矩阵* 左上区域的 3x3 矩阵包含改变方向的**子矩阵**，那我们为什么不直接用法向量左乘这个**子矩阵**？

  当模型发生了non-uniform缩放的时候，经过上述矩阵变换，法线就不再和模型表面垂直了，失去了法线的意义，例如下图：

  ![image-20231210215142420](C++.assets\image-20231210215142420.png)

  那么用于法线变换的矩阵应该长什么样呢?答案是：

  把法线从local space变换到view space的变换矩阵为顶点位置变换矩阵 $MV$ 的逆矩阵的转置矩阵，即 $({MV}^{-1})^{T}$。

  (如果在world space中进行光照计算，则normal matrix为$({M}^{-1})^{T}$。

  * **推导**

  ![image-20231210215257943](C++.assets\image-20231210215257943.png)

  

![image-20231210215325026](C++.assets\image-20231210215325026.png)



## 渲染

* **GPU 渲染一般都是以Tile为基本单元的，为什么？**

[Tile-Based Rendering学习笔记 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/393712805)

所谓Tile，就是将几何数据转换成小矩形区域的过程。光栅化和片段处理在每Tile的过程中进行。Tile-Based Rendering的目的是在最大限度地减少fragment shading期间GPU 需要的外部内存访问量,从而来节省内存带宽。TBR将屏幕分成小块，并在将每个小图块写入内存之前对每个小图块进行片段着色。为了实现这一点，GPU 必须预先知道哪些几何体属于这个tile.因此，TBR将每个渲染通道拆分为两个处理通道：

提高渲染效率和性能。

1. **并行处理：** 瓦片化允许GPU并行处理多个小区域。每个瓦片可以独立处理，这意味着不同的处理单元（如CUDA核心）可以同时处理不同的瓦片，从而加速整个光栅化过程。这种并行处理方式充分利用了GPU的强大并行计算能力。
2. **局部性优势：** 瓦片化可以提高空间局部性，因为相邻像素通常会在相邻的瓦片中处理。这意味着在进行纹理采样、深度测试等操作时，可以更好地利用缓存，减少对全局内存的访问，从而提高数据访问效率。
3. **减少带宽需求：** 将屏幕分成小块，减少了在光栅化阶段需要处理的像素数量，从而减少了内存带宽的需求。这对于高分辨率屏幕和复杂场景来说尤为重要，因为在渲染时需要处理的像素数量可能会非常庞大。‘
4. **偏导计算：**在瓦片内部完成偏导计算，获取梯度信息。

* **从深度图如何还原法线？**

[【知识补充】深度信息还原位置和法线 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/367257314#:~:text=在了解如何用深度,即可，伪代码如下)

如何在一个三角形面片中求法线？ **叉乘嘛！**

在对应空间(世界、视角等)找出来近似的面片，叉乘！！！

一个很大的应用情景是在后处理的阶段，或是计算一些屏幕空间的效果（如SSR、SSAO等），只能获取到一张深度贴图，而不是每一个几何体的顶点数据，很多的计算中却又需要用到世界空间的法线或者是视空间的法线，这时我们就需要通过深度图来重建视空间的法线。

```glsl
vec3 P  = GetViewPos(v2f_TexCoords);
vec3 Pl = GetViewPos(v2f_TexCoords + vec2(-xOffset,0));
vec3 Pr = GetViewPos(v2f_TexCoords + vec2(xOffset,0));
vec3 Pu = GetViewPos(v2f_TexCoords + vec2(0,yOffset));
vec3 Pd = GetViewPos(v2f_TexCoords + vec2(0,-yOffset));
vec3 leftDir = min(P - Pl, Pr - P) ? P - Pl : Pr - P//求出最小的变换量
vec3 upDir   = min(P - Pd, Pu - P) ? P - Pd : Pu - P//求出最小的变换量
vec3 normal = normalize(cross(leftDir,upDir))
```

* **如何解决因为顶点过近和浮点数的精度不足带来的穿模问题**
  * 增加深度缓冲的精度值。
  * 开启深度偏移。渲染物体前，可以通过一定量的偏移来调整深度值。
  * 调整相机的远近裁剪面。
  * 几何着色器可以在顶点和片元处理之间执行操作，可以在此处进行一些深度值的处理，以缓解穿模问题。

## 纹理

### 纹理基础

* **纹理技术的基本原理**

简单的理解就是将一张二维图像，按照一定的映射关系，将每个像素贴合到物体表面的对应位置。纹理技术可以增加物体表面的细节。

* **纹理采样模式**
  * Warp(重复寻址)。最常见的模式，四方连续贴图可以无限延伸用的就是这种算法，即超过1之后重复0到1。
  * Clamp(钳位寻址)。最边缘像素拉伸的效果。
  * Border(边框寻址)。坐标越界后，返回的颜色为给定的颜色值。
  * Mirror(镜像寻址)，类似于重复，只不过是镜像重复效果。
* **mipmap的概念，如何实现？**
  * 提出背景：在一个场景中有多个物体，有远近之分，远的物体只占很少的片段，此时如果要从高分辨率的纹理中采样，会比较困难，一个小物体中的像素映射到了纹理上会占据很大一块，包含了很多个纹理像素，不好直接采样，因此引出了mipmap的概念。
  * 原理：将纹理划分为了不同大小的分辨率图集，每次缩小$\frac{1}{2}$,根据物体的大小，来对不同级别的纹理进行采样，对于远处的物体，采用低分辨率，对于进出的物体，采用高分辨率。
  * 占据额外存储1/3，等比数列。

### 

* **法线贴图及其他贴图的作用**

![image-20231212213133354](C++.assets/image-20231212213133354.png)



### 虚拟纹理

这个概念取自于Virtual Memory,与虚拟内存类似的是，一个很大的Texture将不会全部加载到内存中，而是根据实际需求，将需要的部分加载。与虚拟内存不同的是，它不会阻塞执行，可以使用更高的mipmap来暂时显示，它对基于block的压缩贴图有很好的支持。 基本思路是，会将纹理的mipmap chain分割为相同大小的tile或page,这里的纹理叫虚纹理，然后通过某种映射，映射到一张内存中存在的纹理，这里的纹理是物理纹理，在游戏视野发生变化的时候，一部分物理纹理会被替换出去，一部分物理纹理会被加载。

![image-20231214095101655](C++.assets/image-20231214095101655.png)

虚拟纹理是一种以时间换空间的纹理流送技术，他最大的好处在于我们能够使用多种高分辨率纹理，而不像传统流送一样受到内存和带宽的限制。

**虚拟内存类比**：虚拟存储技术的基本思想时利用大容量外存来扩充内存，产生一个比有限的实际内存空间大得多的、逻辑的虚拟空间，简称虚存，以便能够有效地支持多道程序系统的实现和大型程序运行的需要，从而增强系统的处理能力。**(按需调入的核心思想)**

**虚拟内存工作原理：**当进程开始运行时，先将一部分程序装入内存，另一部分暂时留在外存；当要执行的指令不在内存时，由系统自动完成将它们调入内存的工作；当没有足够的内存时，系统自动选择部分内存（暂不执行的程序）空间，将其中原有的内容交换到磁盘上，并释放这些内存空间供其他进程使用。

 



## 光照模型

### 局部光照模型

![image-20231205114043434](C++.assets/image-20231205114043434.png)

在真实感图形学中，进处理光源直接照射物体表面的光照模型被称为局部光照模型。

* **Lambert漫反射模型**

  特点：

  * 反射强度和观察者角度没关系
  * 反射强度和光线的入射强度有关系

Lambert是光源照射到物体表面后，向四面八方反射，产生的漫反射效果，这是一种理想的漫反射光照模型。 

```cpp
half3 FinalColor;
FinalColor=Kd*dot(Normal,LightDir);//实现Lambert光照模型
FinalColor*=BaseColor;//叠加模型基础颜色
return FinalColor;
```

* **phong模型**

Phong模型由三种反射光组成，反别是漫反射光，环境光，镜面反射光。

Phong模型会有一个问题，在镜面反射会在一些情况下出现问题，特别是物体反光度低的时候，会导致大片的高光区域，会出现断层的情况，这是因为观察向量和反射向量之间的夹角不能大于90°，如果点积为负数，镜面光分量会变成0.0，

![image-20231205115226353](C++.assets/image-20231205115226353.png)

* **Blin-phong模型**

Blin-Phong模型在计算高光的时候，选择用半程向量与法线的夹角来代替反射向量和视角的夹角，这样就解决了上面的问题。

* **cook-torrance**

Cook-Torrance模型兼顾漫反射和镜面反射两个部分。

![image-20231205115712576](C++.assets/image-20231205115712576.png)

这里的 $k_d$是早先提到过的入射光线中**被折射**部分的能量所占的比率，而 $k_s$ 是**被反射**部分的比率。BRDF的左侧表示的是漫反射部分，这里用 $f_{lambert}$来表示。它被称为Lambertian漫反射。
$$
f_{lambert} = \frac{c} {\pi}
$$
BRDF镜面反射公式
$$
f_{cook-torrance} = \frac{DFG}{4(w_0 ·n)(w_i ·n)}
$$
D：法线分布函数：描述的是各个为表面法线的集中程度。

F：菲涅尔项，跟观察方向和法线方向有关，当从掠射角观察时没看到的反射现象就越明显

G：几何自遮挡项，考虑的是微表面之间的相互作用，当一个平面相对比较粗糙的时候，平面表面上的微平面有可能挡住其他的微平面从而减少表面所反射的光线。



* **phong和blin-phong有什么区别？**

都是经验模型，`finalColor = diffuse + specular + ambient`，

1. ambient环境光分量用于模拟全局光照效果，是整体提了一个亮度；

2. diffuse项用到了光线到片段向量与片段平面法线向量的点乘，乘上光的颜色和物体颜色；
3. specular项计算有所不同，phong模型计算的时候用的反射方向和视角方向的点积；blin-phong计算的时候用的半程向量和法线的夹角的点积，解决了高光不连续的情况。





### **全局光照模型**

* **光线追踪**

光线追踪算法通过模拟光的传播方式，即光从光源出发经过若干次反射、折射到达摄像机的过程来实现全局光照的效果。

1. whitted光线追踪

可实现直接光照、镜面反射和折射效果。从摄像机出发发射光纤，打到世界空间最近的物体，找到着色点，着色点向光源发射一条阴影线以寻找焦点，如果交点存在，则意味着物体被挡住了，该着色点处于阴影之中，否则，着色点可以被直接光源照亮。为了实现镜面的反射和折射的效果，当光源命中了一个镜面材质的物体的时候，继续反射或者折射出新的光线，如此递归下去。

![image-20231205121841705](C++.assets/image-20231205121841705.png)

```cpp
Vector3f castRay(
        const Vector3f &orig, const Vector3f &dir, const Scene& scene,
        int depth)//传入原点，方向，屏幕，深度
{
    if (depth > scene.maxDepth) {
        return Vector3f(0.0,0.0,0.0);
    }

    Vector3f hitColor = scene.backgroundColor;
    if (auto payload = trace(orig, dir, scene.get_objects()); payload)
    {
        Vector3f hitPoint = orig + dir * payload->tNear;
        Vector3f N; // normal
        Vector2f st; // st coordinates
        payload->hit_obj->getSurfaceProperties(hitPoint, dir, payload->index, payload->uv, N, st);
        switch (payload->hit_obj->materialType) {
            case REFLECTION_AND_REFRACTION:
            {
                Vector3f reflectionDirection = normalize(reflect(dir, N));
                Vector3f refractionDirection = normalize(refract(dir, N, payload->hit_obj->ior));
                Vector3f reflectionRayOrig = (dotProduct(reflectionDirection, N) < 0) ?
                                             hitPoint - N * scene.epsilon :
                                             hitPoint + N * scene.epsilon;
                Vector3f refractionRayOrig = (dotProduct(refractionDirection, N) < 0) ?
                                             hitPoint - N * scene.epsilon :
                                             hitPoint + N * scene.epsilon;
                //重新在hitPoint生成两条光线
                Vector3f reflectionColor = castRay(reflectionRayOrig, reflectionDirection, scene, depth + 1);
                Vector3f refractionColor = castRay(refractionRayOrig, refractionDirection, scene, depth + 1);
                float kr = fresnel(dir, N, payload->hit_obj->ior);
                hitColor = reflectionColor * kr + refractionColor * (1 - kr);
                break;//
            }
            case REFLECTION:
            {
                float kr = fresnel(dir, N, payload->hit_obj->ior);
                Vector3f reflectionDirection = reflect(dir, N);
                Vector3f reflectionRayOrig = (dotProduct(reflectionDirection, N) < 0) ?
                                             hitPoint + N * scene.epsilon :
                                             hitPoint - N * scene.epsilon;
                hitColor = castRay(reflectionRayOrig, reflectionDirection, scene, depth + 1) * kr;
                break;
            }
            default:
            {
                Vector3f lightAmt = 0, specularColor = 0;
                Vector3f shadowPointOrig = (dotProduct(dir, N) < 0) ?
                                           hitPoint + N * scene.epsilon :
                                           hitPoint - N * scene.epsilon;

                for (auto& light : scene.get_lights()) {
                    Vector3f lightDir = light->position - hitPoint;
                    // square of the distance between hitPoint and the light
                    float lightDistance2 = dotProduct(lightDir, lightDir);
                    lightDir = normalize(lightDir);
                    float LdotN = std::max(0.f, dotProduct(lightDir, N));
                    // is the point in shadow, and is the nearest occluding object closer to the object than the light itself?
                    auto shadow_res = trace(shadowPointOrig, lightDir, scene.get_objects());
                    bool inShadow = shadow_res && (shadow_res->tNear * shadow_res->tNear < lightDistance2);

                    lightAmt += inShadow ? 0 : light->intensity * LdotN;
                    Vector3f reflectionDirection = reflect(-lightDir, N);

                    specularColor += powf(std::max(0.f, -dotProduct(reflectionDirection, dir)),
                        payload->hit_obj->specularExponent) * light->intensity;
                }

                hitColor = lightAmt * payload->hit_obj->evalDiffuseColor(st) * payload->hit_obj->Kd + specularColor * payload->hit_obj->Ks;
                break;
            }
        }
    }
    return hitColor;
}
```



2. path tracing路径追踪

路径追踪的基本思想是从视点发出一条光线，光线与物体表面相交时根据表面的材质属性继续采样一个方向（选择一个随机方向），发出另一条光线，如此迭代，直到光线打到光源上（或逃逸出场景），然后用蒙特卡洛的方法，计算其贡献，作为像素的颜色值；由于单条光路的蒙特卡洛积分肯定会不准确，产生很多噪点，所以一般是单个像素发射多条光线进行路径追踪，一条路径就是视点和场景中各个物体反射交点的连线;基于蒙特卡洛法，在半球上做随机的采样，一般会做重要性采样，引入权重，递归终止条件可以采用俄罗斯轮盘赌的方法，为了提高采样的质量，可以利用积分变换从光源出发发射光线。

```cpp
float shade(vec3 p, vec3 wo)
{
    //直接光部分
    vec3 q = random() by pdf_light;//光源上随机采样一个q点
    vec3 wi = normalize (q - p);
    float l_dir = 0.0;
    ray r2light = ray(p, wi);
    if(r2light hit light at q)//没有障碍物
        l_dir = fr(p, wi, wo) * li * dot(n, wi) * dot(n', wi) / pdf_light(q) / len(q-p)^2;
    
    //间接光部分
    float l_indir = 0.0;
    float prob = 0.6;
    float num = random(0,1);
    if(num < prob)
    {
        vec3 wi = random() by pdf;
        ray r = ray(p, wi);
        // object不能是光源，如果r打在了光源上则忽略。
        if(r hit object at o)
            l_indir = fr(p, wi, wo) * shade(o, -wi) * dot(n, wi) / pdf(wi) / prob;
    }
    return l_dir + l_indir;
}
```

### PRTGI

PRT思想：将光照切割成Lighting, Light Transport两部分。

### RSM(Reflective shadow map)

1.考虑一次间接反射，需要先知道二级光源，即直接光照可以照亮的地方，什么信息可以告诉我们这一点呢？Shadow Map可以，存储了从光源看过去最远的点。

![image-20231209202837621](C++.assets\image-20231209202837621.png)

在此，还有一个假设，被直接光照照亮的物体反射都是diffuse的，这样radiance比较好求。这里是假设reflector，没有要求receiver也是diffuse。

2. 考虑哪些次级光源会对着色点p产生贡献。

考虑所有surface patch的贡献，进行求和；并且每个surface patch可以看成是一个area light。

我们之前说过每一个小的patch都可能对照亮p点做出贡献,因此我们可以先计算出一个patch做出的贡献,之后用求和的形式将所有patch的贡献加在一起.

![image-20231210095239257](C++.assets\image-20231210095239257.png)



我们可以看到q是一个patch去照亮点P

q其实就是RSM中一个Texel所对应的patch,在games101中我们说过,原本计算q对p点的贡献,我们应该是对整个立体角进行采样,但是这样的话很浪费很多的sample,为何不直接在light处采样然后去计算p点的shading值呢.

![image-20231210095536227](C++.assets\image-20231210095536227.png)

也就是把立体角的积分变成了对light区域面积的积分,如果当区域足够小的时候dA甚至不用积分，直接相乘后相加就行,现在我们要解的是patch在接受直接光照后反射出的radiance是多少,也就是从q点到p点的radiance,那么如何解呢?

![image-20231210095552338](C++.assets\image-20231210095552338.png)

对于每个次级光源点来说,由于我们假设它的brdf是diffuse的,因此次级光源的fr积分后是个常数,此时我们把Li代入到式子中会发现dA刚好会被抵消.之后式子会少去dA,会多出来一个$\phi_p$，然后$\phi_p$和 $\frac{cos\theta_q cos\theta_p}{||p-q||^2}$ 又组成了下列公式中的Ep与剩余部分结合求出了一个次级光源p对着色点所得到的shading结果,再将积分域中所有的结果加在一起,就是着色点最后被间接光照照亮所得到的shading值.

![image-20231210100401052](C++.assets\image-20231210100401052.png)

公式求的是次级光源的光线贡献在着色点上的Irradiance,Ep表示次级光源对着色点贡献的入射irradiance.



还有什么问题嘛？

首先关于可见性，次级光源到着色点是否可见呢？需要为每个次级光源算一个shadow map，这显然不太可能！难办，那就别办了。

![image-20231210101303452](C++.assets\image-20231210101303452.png)



显然，不是所有的pixels都会产生贡献，可见性、朝向、距离都是影响因素。

- -Visibility（仍然非常难算）；
- -方向：比如X-1点在SM中记录的是桌子的表面，而且这个表面的点法线方向是朝上的，因此根本不可能照亮X点；
- -距离：因为远处的次级光源贡献很少，通常只要找距离足够近的次级光源就行了。

因此为了加速这一过程,我们认为在shadow map中着色点$x$的位置和间接光源$x_p$的距离可以近似为它们在世界空间中的距离。所以我们认为，对着色点$x$影响大的间接光源在shadow map中一定也是接近的。

于是我们决定先获取着色点$x$在shadow map中的投影位置(s,t)，在该位置附近采样间接光源，多选取一点离着色点近的VPL，并且为了弥补越往外采样数越少可能会带来的问题，引入了权重，越近了权重越小，越远的权重越大。那么对于一个shading point差不多找400个次级光源来计算是比较合适的。如下图所示。

![image-20231210102101582](C++.assets\image-20231210102101582.png)

现在ShadowMap上存储的东西就比较多了，深度值、世界坐标、法线、flux

![image-20231210102352885](C++.assets\image-20231210102352885.png)

优点：容易实现。

缺点：性能会随着直接光源的数量的增加而降低（光源越多，shadow map越多，次级光源越多）

​			没有做可见性检查，并且假设反射物是diffuse的

​			需要在质量和采样率上做一个平衡（采样多，性能低，质量高；采样少，性能高，质量垃；典型trade off）

### LPV(Light Propagation Volumes)

核心思想：在3D空间中传播光线，从而利用它做出间接光照而实现了GI

LPV特点：

1. fast
2. Good quality

问题：如果我们能获得从任何一个shading point上来自四周的radiance的话们就可以立刻得到间接光照。

核心思路：我们假设光在传播的过程中，radiance是uniform的（intensity 平方衰减）

解法：将场景划分为若干个3D网络，每一个网络叫做Voxel，在计算直接光照后，将接收到直接光照的表面看作间接光照在场景中的传播点。

![image-20231210154842986](C++.assets\image-20231210154842986.png)

步骤：

1. 找出接收直接光照的点。
2. 把这些点注入到3D网络中作为间接光照的传播起点。
3. 在3D网络中传播radiance
4. 传播完成后，渲染场景

具体步骤：

* **生成**

  ![image-20231210155053672](C++.assets\image-20231210155053672.png)

  * 首先通过Shadow Map找出接受直接光照的物体表面
  * 对得到的光源数量可以通过采样一些进行简化进而降低次级光源的数量，最后获得一系列的虚拟光源

* **注入**

  ![image-20231210155154301](C++.assets\image-20231210155154301.png)

  * 预先将场景划分为3D网格
  * 将虚拟光源注入到对应的格子中
  * 一个格子内可能含有多个不同朝向的虚拟光源，把各自内所有虚拟光源不同朝向的radiance算出来并sum求和，从而得到一个往四面八方发射的radiance
  * 由于是在空间上的分布，可以看作球面函数，用SH(球谐函数)表示（工业界一般用两阶SH就可以表示各个方向上的radiance初值）

  ![image-20231210155427763](C++.assets\image-20231210155427763.png)

* **传播**

  ![image-20231210155442602](C++.assets\image-20231210155442602.png)

  * 由于是3D网络，因此可以向六个面传播(上下、前后、左右)，由于radiance是沿着直线传播，我们认为radiance是从网格中心往不同方向进行传播的，穿过哪个表面就往哪个方向传播比如穿过右表面的radiance,就传播到右边的格子里(不考虑斜角,比如右上方向,我们认为是先到右边格子,再到上面格子)
  * 每个格子计算收到的radiance,并用SH表示
  * 迭代四五次之后,场景中各voxel的radiance趋于稳定

* **渲染**

  - 对于任意的shading point，找到他所在的网格

  - 获得所在网格中所有方向的Radicae；

  - 渲染

* **问题**

  * 漏光

  由于我们认为radiance是从格子正中心向四周发散的,当遇到这种情况时,

  ![image-20231210163223037](C++.assets\image-20231210163223037.png)

  按理说点P反射的radiance是无法照亮墙壁的背后,但是由于我们的假设,会导致墙壁后面也被间接光照照亮,也就是所谓的漏光现象.

  ![image-20231210163305449](C++.assets\image-20231210163305449.png)

如图,你看房屋的下部本不应该被照亮,但由于使用了LPV导致了light leaking现象.

我们是可以解决漏光现象的,那样需要我们划分的格子足够小,这样会导致存储量增多,而且传播过程中传播的格子量增多,也就导致了速度慢.

对于两个格子之间的可见性也进行了假设，假设相邻格子都能看见，同时工业界会用不同大小的格子 也就是**Cascade层级加速结构**，来优化LPV的方法。



### VXGI(Voxel Global IIIumination)

VXGI也是一个2-pass算法，但是与RSM有一些区别。

* **区别1：次级光源从RSM中的pixel→VXGI中的Voxel（格子）**

RSM 中次级光源是像素中所包含的微小表面，这些表面是根据Shadow Map来划分的.

VXGI把场景完全离散化成了一系列微小的格子，可以理解为场景是由一堆乐高堆起来的,如图,这些是最细的层级,也就是最小的格子我们可以在这一层基础上去建立一层大点的格子,依此类推从而根据场景的不同划分建立出一个Hierachical树形结构的体素。



* **区别2：光线从传播变为了追踪**

在LPV中,我们将受到直接光照的点注入到场景划分的Voxel之后进行传播,只需要传播一次就可以知道场景中任何一个shading point收到间接光照的radiance.

而在VXGI中第二趟我们从camera出发,就像有一个Camera Ray打到每一个pixel上,根据pixel上代表的物体材质做出不同的操作,如果是glossy则打出一个锥形区域,diffuse则打出若干个锥形区域,打出的锥形区域与场景中一些已经存在的voxel相交,这些voxel对于Shading point的贡献可以算出来,也就是我们要对每一个shading point都做一个cone tracing

![image-20231210165921762](C++.assets\image-20231210165921762.png)

* **具体步骤**

  * **Pass1：Light Pass**

  首先找出来那些Voxel会被照亮，那么我们要从接收到直接光照的patch开始，不管是RSM还是什么，先找出来受直接光照影响的patch。

  但是场景现在是由voxel表示的，那么对于任何一个格子，跟LPV注入很像，这里不再记录表面的出射分布或者说认为表面是diffuse的半球分布，也就是不再像LPV一样将所有的radiance加在一起求一个各方向的初始值。

  ![image-20231210165318801](C++.assets\image-20231210165318801.png)

  记录的是直接光源从哪些范围来（绿色部分），记录各个反射表面的法线（橙色部分），通过**输入方向**和**法线范围**两个信息然后通过表面的材质，来准确的算出出射的分布，这样就比LPV认为格子表面是diffuse再用SH来压缩的方法要准确，然后建立更高层级格子的这些特性。

  * **Pass2：Camera Pass**

从这一步开始考虑场景的渲染了,对于任何一个像素，知道了Camera Ray的方向，

**I)** 对于Glossy的表面，向反射方向追踪出一个锥形(cone)区域；

![image-20231210165554629](C++.assets\image-20231210165554629.png)

基于追踪出的圆锥面的大小，对格子的层级进行查询，就是对于场景中的所有体素都要判断是不是与这个锥形相交，如果相交的话就要把对于这个点的间接光照的贡献算出来(我们存储了体素的光照输入方向和法线方向,因此可以算出其输出的radiance,将cone区域内所有体素的radiance都算出来从而在shading point得到间接光照)，也就是根据传播出的距离远近找对应层级的体素，然后找覆盖的范围。

**II)** 对于diffuse的情况来说,通常考虑成若干圆锥，忽略圆锥Tracing时的重叠和空隙。

![image-20231210165700537](C++.assets\image-20231210165700537.png)

* **总结**

LPV是把所有的次级光源发出的Radiance传播到了场景中的所有位置，只需要做一次从而让场景每个Voxel都有自己的radiance，但是由于LPV使用的3D网格特性，并且采用了SH进行表示和压缩，因此结果并不准确，而且由于使用了SH因此只能考虑diffuse的,但是速度是很快的。

VXGI把场景的次级光源记录为一个层次结构，对于一个Shading Point，我们要去通过Corn Tracing找到哪些次级光源能够照亮这个点。



## 抗锯齿处理

### 锯齿产生原因

屏幕是由一系列离散的点组成，光栅化的时候，是以像素中心点是否被三角形覆盖来决定是否生成片段，就产生了锯齿。

### SSAA(超采样抗锯齿)

先映射到高分辨率缓存中，然后对每个像素图像像素采样，一般取临近2-4个像素，采样混合后，生成最终的像素再缩小到原来图像一样的大小。

加入屏幕分辨率为 M*N，超采样次数为k，每个像素都会进行k次着色

### MSAA(多重采样抗锯齿)

MSAA运行在光栅化阶段，会计算一个覆盖率，在片段着色阶段，每个像素仍然只运行1次着色，只是最后的结果会乘上覆盖率。

缺点：MSAA不支持延迟渲染，MSAA本质上是发生在光栅化阶段的技术，需要用到场景的几何信息，延迟渲染着色计算的时候已经丢失了该信息，如果强行这么处理，MSAA会增加数倍的带宽，也是一个非常严峻的问题，

* MSAA一个点在三角形内部就最后相乘的覆盖率就是100吗？

**还要考虑深度缓冲。**

### FXAA(近似快速抗锯齿)

FXAA是MSAA一种高性能近似，位于后处理阶段实现，不依赖于硬件，总体思想：

1. 找出来图像中的所有边缘(通过亮度比较，G分量)
2. 平滑化边缘(沿着某一个方向将一定范围的像素取出来求加权平均)

### TAA(时域抗锯齿)

从时间维度上进行抗锯齿处理，使用同个像素在不同帧上的不同采样点，根据时间先后进行一个加权平均计算；但是TAA也有缺点，就是容易出现鬼影和抖动的现象；

## 阴影技术

对于静态的物体可以采用LightMap烘焙的方法来获取物体的影子，而对于动态物体，一般采用的是shadowmap技术。

### LightMap(光照贴图)

（1）原理：从光源的方向去烘培(离线渲染)一个物体，把结果存一张贴图里，因为离线渲染的时候，如果光线和物体之间有东西被遮挡，那么物体上该点处就会存在阴影，那么在Lightmap上就是一个阴影的值(较暗的像素)**，**然后渲染的时候直接对该物体从光照贴图里面采样即可，

（2）缺点：Lightmap只能存diffuse分量，不能存specular分量，没办法做动态阴影。

### ShadowMap(阴影贴图)

从光源渲染一遍场景，将深度信息存到深度图中，在正常渲染一次场景，利用shadowmap来判断那些片段落入了阴影中。

* 常见问题

  * 抖动(自遮挡)

  可以通过偏移来解决，增加一个bias来比较片段深度，还有一种更好的方式是自适应偏移，基于斜率去计算当前深度要加的偏移。

  * 锯齿

  可以使用百分比渐进过滤(Percentage Closer Filter，PCF)技术进行解决：从深度贴图中多次采样，每次采样坐标都稍有些不同，比如上下左右各取9个点进行采样（即一个九宫格），最后加权平均处理，就可以得到柔和的阴影。标准PCF算法采样点的位置比较规则，最后呈现的阴影还是会看出一块一块的Pattern（图块），可以采用一些随机的样本位置，比如Poisson Disk来改善PCF的效果

### PCSS（Percentage-Closer Soft Shadows）

PCF filter的半径是固定的，但是根据自然现象来看，遮挡物和阴影距离越近，阴影应该越硬

![image-20231205132101004](C++.assets/image-20231205132101004.png)

根据这样的现象，PCSS通过相似三角形原理，动态计算出PCF应该采样的范围大小：

![image-20231205132238307](C++.assets/image-20231205132238307.png)



### CSM(Cascade ShadowMap，级联阴影贴图)

CSM根据对象到观察者的距离距离提供不同分辨率的深度纹理来解决上述问题。CSM将相机的谁锥体分割成了若干部分，然后为分割的每一个部分生成独立的深度贴图。

CSM通常用于大型场景模拟太阳的投射，对于**近处**的场景使用**较高分辨率**的**阴影贴图**，对于**远处**的场景使用**粗糙**的**阴影贴图**。

首先要生成CSM：

1.对视锥体进行划分

![image-20231205213522424](C++.assets/image-20231205213522424.png)

2. 每一个子视锥体都是一个场景，所以我们每一级Shadow map都要至少覆盖整个视锥体，阴影相机通常使用正交投影，正交投影的视野范围是长方体故不难想到通过 AABB 盒子来包围：

![image-20231205213707322](C++.assets/image-20231205213707322.png)

3. 生成深度图，有了视锥划分的box之后，我们还需要设置阴影相机以生成深度贴图，阴影相机的正交范围和包围盒保持一致。

采样思路如下：

1. 使用每个光源的**光椎体**渲染**场景**的**深度值**。
2. 从相机位置渲染场景。根据**片段**的**z值**，选择**合适**的**阴影贴图**查询片段对应的**阴影贴图**中的**深度数据**，将其和**片段**在**光椎体**下的**深度值**进行比较，根据比较结果决定**片段**的**最终颜色**。

CSM明显存在抖动问题：

* 相机的平移抖动，在移动的时候，对场景的同一个三角形采样出来的深度图不一样，我们可以通过控制每次偏移的距离来解决这个问题，保证每次偏移都是深度图 空间长度/resolution的整数倍

```c#
// 计算 Box 中点, 宽高比
Vector3 center = (box[3] + box[4]) / 2;
float w = Vector3.Magnitude(box[0] - box[4]);
float h = Vector3.Magnitude(box[0] - box[2]);
//float len = Mathf.Max(h, w);
float len = Vector3.Magnitude(f_far[2] - f_near[0]);
float disPerPix = len / resolution;

Matrix4x4 toShadowViewInv = Matrix4x4.LookAt(Vector3.zero, lightDir, Vector3.up);
Matrix4x4 toShadowView = toShadowViewInv.inverse;

// 相机坐标旋转到光源坐标系下取整
center = matTransform(toShadowView, center, 1.0f);
for (int i = 0; i < 3; i++)
    center[i] = Mathf.Floor(center[i] / disPerPix) * disPerPix;
center = matTransform(toShadowViewInv, center, 1.0f);
```

* 相机的旋转抖动

在旋转的时候相机的包围盒长宽时刻在变化，单张阴影贴图能覆盖的面积也不断变换，自然造成了生成图像的走样：

![image-20231128165425079](C++.assets/image-20231128165425079.png)



因为我们的包围盒设定上就是要严密包围视锥体，所以对旋转变换比较敏感。可以粗暴的将主相机子视锥体（梯形台）的长对角线作为包围盒的宽高，因为只要主相机参数和 CSM 划分参数不变那么长对角线就不会变，况且包围盒最长不会超过长对角线。

这样一来能够保证正交投影视锥体的宽高和主相机旋转无关。这么做虽然损失一些阴影贴图的精度但仍是合理的交换，况且低精度的 Shadow Map 我们有各种 trick 来料理它。再次修改 CSM.ConfigCameraToShadowSpace 函数，首先取得当前 level 的子视锥体（梯形台）然后计算长对角线。



## 延迟渲染

* **什么是延迟渲染？G-Buffer要存什么东西？**

（1）延迟渲染要将物体的几何信息(位置、法线、颜色、镜面值)存到集合缓冲区中(G-Buffer)，在光照处理阶段，使用G-Buffer中的纹理数据，对每个片段进行光照计算，这种渲染方法的一个好处就是能保证在G-Buffer中的片段和屏幕上呈现的像素所包含的片段信息是一致的，因为深度测试已经最终将这里的片段信息作为最顶层的片段。这样保证了在光照处理阶段，每个像素只处理一次，延迟渲染思想：先深度测试，再着色计算，将本来在物体空间(三维空间)进行的光照计算放到了屏幕空间(二维空间)处理计算。

（2）在每一帧当中G-buffer存储的信息有：位置、法线、颜色值、镜面值（所以其实有三张纹理，分别存位置、法线和颜色+镜面值(RGB+)A)；如果是PBR，应该还要再存一个金属度和粗糙度贴图。

* **延迟渲染和正向渲染的区别？优缺点**
  * 区别：正向渲染先执行着色计算，再执行深度测试；光源数量对计算复杂度影响很大，一个像素运行多次片段着色器；延迟渲染先执行深度测试，后执行作色计算，对于延迟渲染，每个像素只会执行一次片段着色器。
  * 延迟渲染支持多光源场景，后处理也比较容易，计算复杂度低，但是不支持MSAA抗锯齿，不支持透明物体渲染，显存开销比较大。
* **G-Buffer使用不同的shader怎么处理？**

比如有不同的shader处理不同的物体，地形有对应的地形shader，模型有对应的模型shader。

1. 可以考虑用模板，写入GBuffer的时候，根据不同位置对模板写入不同的值，例如模型写入1，地形写入2，shading的时候采用模板测试。
2. 根据移动端架构，每一个图元都有一个对应的tile，类似这种架构，每一个图元记录上对应的shader信息？(瞎几把写的)



* **延迟渲染性能上最大的缺点？有没有什么优化的办法？**

缺点：带宽大，不支持透明物体渲染，通过压缩gbuffer大小来优化。

优化方法：

​	1. GBuffer的压缩(CryEngine的MicroGBuffer)

	2. 苹果的 metal 有实现移动端 gbuffer 的优化，通过 Lossy compression 压缩 Render Target 的大小以节省显存带宽

* **常见纹理贴图压缩算法**

游戏绘制中图片压缩不可以使用jpg等的压缩算法，因为算法复杂并且不能拿取特定位置的数据
游戏中基本使用基于块的压缩算法:PC-BC7,DXTC;Mobile:ASTC,ETC,PVRTC

这里介绍一个非常经典的算法。对于DXT类型的纹理，在一个4×4的色块中，可以找到最亮的点和最暗的点，即颜色最鲜艳和颜色最暗的点，然后将该方块中的其他点都视为这两个点之间的插值。因为对于很多图片来说，相邻的像素之间都有一定的关联度（Coherence）。所以我们可以存储一个最大值和一个最小值，然后为每个像素存储一个距离最大值和最小值的比例关系，这样就可以近似地表达整个色块中的每个像素的颜色值。**在计算机图形学领域中，纹理压缩（Texture Compression）都是基于这个思想，称为块压缩（Block Comppression）。**在DirectX中，最经典的就是DXT系列的压缩算法。块压缩系列压缩算法的最新版本已经演进到了BC7。DXT系列压缩算法的优势在于，当我们生成了一个纹理后，就可以在CPU上对纹理进行实时压缩。因为无论是压缩还是解压缩，这一系列算法的效率都非常高。

另外一类压缩算法就是手机上使用的压缩算法，使用较多的就是ASTC算法。ASTC压缩的分块就不再是严格的4×4了，它可以使用任意的形状，而且ASTC的压缩效果是最好的，解压缩的效率也不低。然而，ASTC算法压缩时的性能消耗较大，因此无法在运行中进行压缩。总而言之，对于计算机的渲染系统来说，纹理压缩的基本逻辑都是按照这个思想来进行压缩的。

![image-20231214110325731](C++.assets/image-20231214110325731.png)



## 空间加速结构 & 算法

### AABB包围盒

把物体放在一个三个轴向对其的包围盒内(一个矩形)，如果光线无法与包围盒相交，那必然也无法和包围盒里面的物体相交，这样就可以省略大量不必要的求交计算；而且光线与包围盒的求交计算与具体的物体求交相比，速度是快得很多的；使用AABB的好处，计算可以简化，很容易计算出t，只需要用轴分量计算。

![image-20231212213323559](C++.assets/image-20231212213323559.png)

### 均匀格子法（uniform grids）

在AABB里面划分小格子，然后预处理将物体包含的格子做标记；接着遍历光线上的格子，在计算光线与标记过的格子中的物体是否相交。这样可以省略对那些不包含格子的物体进行求交计算，进一步提高求交速度：

![image-20231212213518207](C++.assets/image-20231212213518207.png)

### 层次包围盒(BVH)

基于对象进行划分，目前得到最广泛的应用：

![image-20231212213724962](C++.assets/image-20231212213724962.png)

使用BVH可以避免一个对象同时存在不同的包围盒/区域中，以下是根据空间划分和根据对象进行划分的算法区别：

![image-20231212213915921](C++.assets/image-20231212213915921.png)





## 数学

### 蒙特卡罗积分与重要性采样

* **蒙特卡洛积分**

这是一种以高效的离散方式对连续的积分求近似的非常直观的方法：对于任何面积/体积进行积分，例如半球 Ω ——在该面积/体积内生成数量 N 的随机采样，权衡每个样本对最终结果的贡献并求和。

![image-20231205215526007](C++.assets/image-20231205215526007.png)

* **重要性采样**
  * 原理：重要性采样即通过现有的一些已知条件(分布函数)，想办法集中于被积函数分布可能性较高的区域(重要的区域)进行采样，进而可高效计算估算结果的一种策略。
  * 理解：因为概率密度函数可能不是均匀分布的，有些地方概率高，因此应该尽可能的多采用概率密度高的区域。
  * 举例：在使用路径追踪的时候，我们会随机生成一条反射光线，如果这个光线是均匀分布的话，很有可能可能许多发射出的光线最后都没有与光源相交，这样就造成了很多计算的浪费。重要性采样是说，着重去采样那些更有可能打到光源上的光线，比如更多地采样光源方向的光线：

![image-20231205215919070](C++.assets/image-20231205215919070.png)

![image-20231205215935698](C++.assets/image-20231205215935698.png)

###  三维数学

* **变换矩阵的作用和推导**

* **欧拉角、矩阵、四元数的区别和优缺点**

（1）欧拉角：定义了绕着三个坐标轴的旋转角，来确定刚体的旋转位置的方式，包括俯仰角pitch，偏航角yaw和滚动角roll；它的优点是比较直观，而且单个维度上的角度也比较容易插值；缺点是它不能进行任意方向的插值，而且会导致万向节死锁的问题，旋转的次序对结果也有影响

（2）矩阵：优点是不受万向节死锁的影响，可以独一无二的表达任意旋转，并且可以通过矩阵乘法来对点或矢量进行旋转变换；现在多数CPU以及所有GPU都有内置的硬件加速点积和矩阵乘法；缺点是不太直观，而且需要比较大的存储空间，也不太容易进行插值计算。

（3）四元数：四元数的好处是能够串接旋转；能把旋转直接作用于点或者矢量；而且能够进行旋转插值；另外它所占用的存储空间也比矩阵小；四元数可以解决万向节死锁的问题。



* **如何判断一个点在三角形(矩形、扇形)内**

（1）面积法，点划分的三个小三角形面积是否等于大三角形；

（2）叉乘法，沿逆时针方向，三角形两两顶点构成三个向量，比如AB,BC,CA，分别用这三个向量与起点和P的交点构成的向量求叉乘，如ABxAP, BCxBP, CAxCP，由右手定则，如果三个结果都是正的，说明这个点都在向量的左边；可以推导得出这个点在三角形内，否则只要有一个是负数，就说明在右手边，在三角形外了。



* **如何判断一条光线是否与一个三角形相交**

（1）先判断光线是否和三角形所在的面相交，再判断这个交点是否在三角形内，判断点是否在三角形内；

（2）用Moller Trumbore算法，简称MT算法。（光线的方程是：ray = origin + direction * t) 原理是如果一个点在三角形内，就能用重心坐标系去表示这个点；重心坐标公式，α = 1 – β - λ；带入方程，有3个未知数（β，λ，t），由因为都是三维变量，可以得到三个等式；利用克拉默法则，线性代数的知识，就可以求解出这三个未知数；解出来，判定t是否合理，t > 0，然后α、β和λ三个系数都是非负的，就是有解，在三角形内。



* **如何判断两个三角形是否相交**

如果两个三角形相交，必定至少其中一个三角形的一条边穿过了另一个三角形的内部；把边当作光线去跟另一个三角形求交，如果三条边只要有一条边跟三角形有交点，即可判断两三角形相交，当然关键是要判断计算出来的t是否合理，是否位于边长范围之内。

## 杂项

**6.1 什么是齐次坐标，齐次坐标有什么作用？**

（1）所谓齐次坐标就是为矢量或者矩阵增加一个维度，2D平面使用3维向量和三维矩阵，3D空间使用4维向量和4维矩阵；额外的坐标值是任意的，可以看作缩放或者权重。

（2）三维矩阵可以表示旋转和缩放，它们相乘的结果是正确的，但是平移变换不能加到三维矩阵中的相乘去表达，只能将矩阵相乘的结果加一个三维向量；引入齐次坐标之后会增加一个维度，变为四维矩阵，多出来的一维向量用来表示平移，那么就可以在一个矩阵中统一所有的操作：平移、旋转、缩放。



**6.2 什么是副法线(切线)，有什么用？**

副法线是法线与切线的叉乘得来，法线、切线和副法线可以定义切线空间，有了切线空间可以很方便地进行法线贴图的计算(当然也有其他的用途)。



# 简历

## C++

* **什么是面向对象编程思想**

  * **封装**

  封装是指一种将抽象性函式接口的实现细节部份包装、隐藏起来的方法。简单来说，就是将一个对象共有的属性和行为抽离出来封装成一个类。

  * **继承**

  继承是子类继承父类的特征和行为，使得子类对象（实例）具有父类的实例域和方法，或子类从父类继承方法，使得子类具有父类相同的行为。简单来说，一个类可以继承另一个类，子类可以拥有父类所有可以访问的字段和方法。

  * **多态**

  多态是同一个行为具有多个不同表现形式或形态的能力。简单来说，是同一个接口，使用不同的实例而执行不同操作。多态还分为静态多态和动态多态，静态多态的体现主要是方法重载和模板，动态多态体现在方法虚函数重写，父类接收不同子类的实例，接口接收不同实现类的实例。

  * **抽象**

  抽象是一种过程，在这个过程中，数据和程序定义的形式与代表的内涵语言相似，同时隐藏了实现细节，一个概念或者想法不和任何特定的具体实例绑死。简单来说，就是把东西抽离出关键特性就是抽象。

  

## 图形

### 光栅化与光线追踪

* **光栅化原理**

光栅化就是把东西话在屏幕上的一个过程，光栅化渲染管线主要分为应用阶段、几何处理阶段、光栅化阶段和像素处理阶段。

![image-20231206210905619](C++.assets/image-20231206210905619.png)

需要CPU和GPU配合去完成，首先在应用阶段，会完成加载场景、设置渲染状态、设置纹理等等，最后下达一个draw call的指令上交到命令缓冲队列中，GPU会取出命令依次进行处理，来到GPU阶段，首先会进行几何处理，在顶点着色器中，完成MVP的运算等等，还可能会有曲面细分着色器、几何着色器等等生成更加精密的模型信息，然后完成裁剪、投影、屏幕映射等，于是到了光栅化阶段，光栅化会完成插值，确定那些像素要被绘制，然后到了像素着色器进行光照运算等等，最后通过一些测试深度测试、模板测试、混合等操作，我们就得到了屏幕上的一张渲染图片结果。

* **光线追踪原理**

从摄像机出发，根据矩阵逆变换，得到光线的方向，追踪每条光的传播行为，计算对人眼的贡献值。

最早有whitted syle ray tracing，发射光线打到场景中的物体，从物体向光源连线来确定可见性，当碰到反射和折射材质的时候就会递归处理，如果是diffuse就会停下，但是这显然是有问题的，于是又有path tracing，当打到漫反射物体的时候，随机发射一条光线继续递归，推出条件一般用俄罗斯轮盘赌的方法来确定。

* **两者的区别**

两者都是通用的渲染技术

![image-20231206212718481](C++.assets/image-20231206212718481.png)

### PBR与IBL

* **什么是PBR**

  PBR是基于物理的渲染，主要基于微平面理论和一些物理原理，可以更加准确的表达物体和光的交互方式，模型一般采用cook-torrance模型。

  ![image-20231206214708893](C++.assets/image-20231206214708893.png)

  

  推导：

  * 漫反射项

  ![image-20231206215033526](C++.assets/image-20231206215033526.png)

  

  由于假设入射光是均匀且遍布整个半球方向，因此Li与Lo相等，则：

  ![image-20231206215117269](C++.assets/image-20231206215117269.png)

  考虑到能量吸收，反射率，得到最终的漫反射BRDF：

  ![image-20231206215140187](C++.assets/image-20231206215140187.png)

  * **镜面反射项**

  

  ![image-20231206215152896](C++.assets/image-20231206215152896.png)

  D：法线分布函数：描述的是各个为表面法线的集中程度，一般用GGX。

  F：菲涅尔项，跟观察方向和法线方向有关，当从掠射角观察时没看到的反射现象就越明显

  G：几何自遮挡项，考虑的是微表面之间的相互作用，当一个平面相对比较粗糙的时候，平面表面上的微平面有可能挡住其他的微平面从而减少表面所反射的光线，一般也用GGX。

  

  

  目前PBR材质主要有两种工作流，一种是Metallic/Roughness(金属/粗糙度)，一种是Specular/Glossiness(镜面反射/光泽度)。

  * **MR工作流**

  Base Color（SRGB空间） + Metallic(线性空间) + Roughness(线性空间) + AO + Normal + height

  * **SG工作流**

​	   Diffuse（SRGB空间） + Specular(SRGB空间) + glossiness + AO + Normal + height



* **什么是IBL**

  IBL(Image based lighting)是将周围的环境视为一个大光源，IBL通常使用环境立方贴图，我们将贴图中的像素视为光源，对物体产生作用。

  反射方程：

  ![image-20231206214359394](C++.assets/image-20231206214359394.png)

  

  我们的主要目标是计算半球 Ω 上所有入射光方向 wi的积分。

  IBL主要分为两项，漫反射IBL与镜面反射IBL，都可以提前预计算好，在光照计算的时候直接采样。

  * 漫反射IBL预计算

  与视角方向无关，且brdf项为常数，因此可以提到积分表达式外部，在半球区域内均匀采样，最终平均值即可。

  * 镜面反射IBL预计算

  将积分项拆成了两项，第一部分用来计算预滤波环境贴图，第二部分计算brdf信息，生成一张查找纹理。

  在计算预先滤波卷积的时候，这次考虑了粗糙度，粗糙度越大，会导致反射更模糊，因此可以用mipmap去存储，有假设了视角方向总是等于输出采样方向w0。

  和之前卷积环境贴图类似，我们可以对 BRDF 方程求卷积，其输入是 n 和 ωo 的夹角，以及粗糙度，并将卷积的结果存储在纹理中。我们将卷积后的结果存储在 2D 查找纹理（Look Up Texture, LUT）中，这张纹理被称为 BRDF 积分贴图，稍后会将其用于 PBR 光照着色器中，以获得间接镜面反射的最终卷积结果

###  **阴影**

* 生成过程：

  1. 从光源位置渲染场景，得到一张深度图信息
  2. 从摄像机出发，正常的渲染场景
  3. 将场景变换到光源视角下，判断深度信息是否匹配。

* 常见问题：

  1. 阴影抖动(摩尔纹)，可以通过偏移技术，增加一个bias比较片段深度，还有一种是自适应偏移的方案，基于斜率去计算当前深度要加的偏移。
  2. 阴影锯齿，可以采用PCF来改善效果，能在一定程度上模糊软化阴影，但是根据物理实际现象，阴影的软硬程度也和阴影和遮挡物之间的距离有关系，PCSS就考虑了这一点，先通过一个filter去计算平均遮挡物距离，根据平均遮挡物距离求出在shadowmap上的filter大小，以达到此效果。

  在项目中，unity srp实现的pcss，搜索平均遮挡物的距离的时候，给了一个光源的尺寸和csm的维度大小，越靠近相机，csm维度越小，searchFilter会更大。
  $$
  searchFilter = \frac{lightSize}{csmWidth}
  $$

* CSM

  对视锥体区域进行划分，近处区域用比较高的分辨率的阴影贴图，远处区域用于比较粗糙的阴影贴图。

  Shadowmap对于大型场景渲染显得力不从心，很容易出现阴影抖动和锯齿边缘现象。对于室外大场景的实时阴影，可以使用CSM技术。

### **后处理算法**

* Bloom 泛光

OpenGL：

`提取高光区域👉模糊👉合并`。但是这么做效果并不好。

高质量泛光：

`提取高光区域👉降采样👉上采样+叠加`

降采样(mipmap)是为了迅速模糊，达到泛光的“泛”的效果，但是还不够亮，要使得它足够的亮，可以将mipmap相加，mipmap等级低的负责中心高亮，mipmap等级高的负责周边的泛，直接相加的话会导致pattern出现，因此可以上采样，边模糊边上采样。

![image-20231207112210011](C++.assets/image-20231207112210011.png)

* Chromatic Abberation(色差)

求出与中心点的偏移量，根据偏移量采样不同坐标的r、g、b值

```cpp
	vec2 diffFromCenter = TexCoords - vec2(0.5, 0.5);
	vec2 offset = diffFromCenter * texel_size * intensity;

	// Emulated how Unity handles their "fast" implementation
	float r = texture2D(input_texture, TexCoords - (1 * offset)).r;
	float g = texture2D(input_texture, TexCoords - (2 * offset)).g;
	float b = texture2D(input_texture, TexCoords - (3 * offset)).b;
```

* Vignette(晕影)

 通过距离求出一个权重，越靠近中心权重越大，越靠近边缘权重越小，给一个背景颜色，然后将两者混合。

```cpp
vec2 uv = TexCoords;
uv *= 1.0 - TexCoords.xy;
float vig = uv.x * uv.y * 15.0;
vig = pow(vig, intensity * 5.0);

FragColour = vec4(mix(color, sceneColour, vig), 1.0);
```

* Film Grain(电影颗粒)

  根据时间、纹理位置生成一些随机值，就是颗粒的效果，最终和场景图混合。

```cpp
vec3 colour = texture2D(input_texture, TexCoords).rgb;

float x = (TexCoords.x + 4) * (TexCoords.y + 4) * ((time + 1) * 10.0);
vec4 grain = vec4(mod((mod(x, 13.0) + 1.0) * (mod(x, 123.0) + 1.0), 0.01) - 0.005) * intensity;

FragColour = vec4(colour + grain.xyz, 1.0);
```

### **AO**

#### SSAO

前提：**基于光照来自于无穷远处的假设**

把当前视点下的深度缓存当成场景的一个粗略的近似来计算AO , 因为它们都是基于场景在屏幕空间的一个特定表达 , 而 AO 计算也是在屏幕空间中进行的 。

![image-20231207114916762](C++.assets/image-20231207114916762.png)

1. 在以 p 点为中心、 R 为半径的球体空间内( 若有法向缓存则为半球体空间内 ) 随机地产生若干三维采样点
2. 估算每个采样点产生的 AO : 计算每个采样点在深度缓存上的投影点 , 用投影点产生的遮蔽近似代替采样点的遮蔽。

#### SSDO

前提：基于光照来自于非常近的地方，考虑到了间接光照

* **思路**

  * 我们不去假设间接光照是固定不变的
  * RSM中我们用shadow map去找到接收直接光照的点当作间接光照为其他的Shading point提供直接光照,也就是说**我们一定程度上是可以已经得到间接光照的信息。**

  ![image-20231210175232178](C++.assets\image-20231210175232178.png)

  通过AO和DO的对比我们可以看到,AO能够产生变暗的效果使得物体相对感更强烈,但AO并不能做到Clolor Blending（不同颜色的Diffuse会互相照亮）

  * 在SSDO中，我们使用直接光照的信息，但不是从RSM中获得的，而是从screen space中得到的。

* **做法**

![image-20231210175432168](C++.assets\image-20231210175432168.png)

SSDO的做法于path tracing很像,假设在Shading Point的P点，随机的往某一个方向打出一根光线:

1. 如果光线没碰到物体，则认为P点这里接收直接光照
2. 如果碰到了一个点Q,那么算出Q点接受的直接光照打到P点的贡献,从而求出P点的间接光照。

![image-20231210175526106](C++.assets\image-20231210175526106.png)

我们可以发现,SSAO和SSDO是完全相反的两个假设:

AO：在AO中我们认为红色的框里能接收间接光照，黄色框里无法接收间接光照，然后求出加权平均的visibility值,也就是**假设间接光照是从比较远的地方来的**；

DO：在DO中,我们认为红色框里接收的是直接光照,而黄色框里才是接收到的间接光照.因为红色框里的光线打不到用来反射的面，因此这些方向上就不会有间接光照，黄色框里的光线能打到物体上，P点接收到的是来自红色框的**直接光照**+黄色框里的**间接光照**,也就是**假设间接光照是从比较近的反射物来的。**

其实这两个假设都不是完全正确的，物理真实的情况是这两种的混合：近处的是DO，远距离是AO，因此AO与DO也并没有矛盾。

回到渲染方程上,将没有遮蔽的与遮蔽的方向上的光照分开考虑，那么对于DO如何解Rendering Equation:

​	1. 当V=1时是直接光照，而DO的计算是计算间接光照的，因此这个我们完全不用去计算与考虑

![img](https://pic4.zhimg.com/80/v2-447958dc017b9152f168f7df7cbac5e7_1440w.webp)

​	2. 当V=0时也就是间接光照的情况，这个是我们需要关注与计算的。

![img](https://pic1.zhimg.com/80/v2-abe6005d997b5f06ac9bfd9ccad6dda4_1440w.webp)

SSDO的核心是要找哪些patch会被挡住，也就是对点P提供间接光照贡献的是哪些点，做法是与AO完全一样的。

我们同样考虑点P法线部分的半球，判断从P点往A、B、C、D四个方向看会不会被挡住，由于是屏幕空间的算法,因此这里我们同样不考虑在3D场景中A,B,C,D四点会不会与P连成光线,只考虑从camera看去A,B,C,D与P连成的光线会不会被挡住。

这里A/B/D这三个点的深度比从camera看去的最小深度深,也就是说PA,PB,PD方向会被物体挡住,因此会为P点提供间接光照。然后把我们用在RSM中讲的计算间接光照的方法这些点对P的贡献加起来。

![image-20231210175907846](C++.assets\image-20231210175907846.png)

SSDO也会出现一些问题,如下图是假设与实际情况不同的情况，因为我们是在屏幕空间处理的,因此在A点虽然会被canmera看不到，但是AP之间是不会挡住的,实际上A点需要提供间接光照给P点,但在SSDO算法中则不提供。

![image-20231210175942584](C++.assets\image-20231210175942584.png)

从计算量上来看与SSAO差不多，但是不同之处是，判定会被挡住的时候，会额外计算被挡住的小片的贡献，质量非常接近离线渲染。

* **问题**

P点对于半球上的点可见性是通过Camera对这些点的可见性来近似计算的，存在于屏幕空间中丢失信息的问题，下图是一个很明显的例子，当黄色的面朝向屏幕的时候地面的SSDO信息是正确的，而当旋转过去之后，就看不到SSDO的信息了。

![image-20231210180059357](C++.assets\image-20231210180059357.png)

SSDO只能解决一个很小范围内的全局光照，下图是接近正确的情况，而如果使用SSDO来计算，方块右边是追踪不到远处绿色的墙的，方块上也就不会有绿色的反光。

![img](https://pic1.zhimg.com/80/v2-7924f04b10d02afcb698f829918b305c_720w.webp)



### **抗锯齿**

* **FXAA**

首先找出来边缘，可以根据亮度公式，相差比较大的就认为是边缘信息，对于边缘信息就要确定混合的方向，然后计算出对应的混合因子.

* **MSAA**

发生在光栅阶段，通过判断点是否在三角形内，来计算一个初步的覆盖率，像素着色器只运行一次，最后结果与着色率相乘。

* **SSAA**

超采样，先生成一张高分辨率的图像，再做降采样。









## OpenGL渲染器

### 渲染队列

* 分为Opaque队列和Transparent队列，先渲染Opaque队列，再对透明物体进行渲染。

* 顺序: Opaque队列->天空盒->透明物体排序->Transparent队列

* 队列中存放的是RenderModel，Model中可拥有多个Mesh，每一个Mesh都有一个Material，Material采用PBR MR工作流程

### PBR

PBR渲染，采用的是MR工作流程，在Material中绑定各个材质，名字统一命名为`texture_albedo(normal metallic roughness ao等等)`

```cpp
void Material::BindMaterialInformation(Shader *shader) const{
		// Texture unit 0 is reserved for the shadow map
		// Texture unit 1 is reserved for the irradianceMap used for indirect diffuse IBL
		// Texture unit 2 is reserved for the prefilterMap
		// Texture unit 3 is reserved for the brdfLUT
		int currentTextureUnit = 4;

		shader->setUniform("material.texture_albedo", currentTextureUnit);
		if (m_AlbedoMap) {
			m_AlbedoMap->bind(currentTextureUnit++);
		}
		else {
			TextureLoader::getDefaultAlbedo()->bind(currentTextureUnit++);
		}

		shader->setUniform("material.texture_normal", currentTextureUnit);
		if (m_NormalMap) {
			m_NormalMap->bind(currentTextureUnit++);
		}
		else {
			TextureLoader::getDefaultNormal()->bind(currentTextureUnit++);
		}

		shader->setUniform("material.texture_metallic", currentTextureUnit);
		if (m_MetallicMap) {
			m_MetallicMap->bind(currentTextureUnit++);
		}
		else {
			TextureLoader::getDefaultMetallic()->bind(currentTextureUnit++);
		}

		shader->setUniform("material.texture_roughness", currentTextureUnit);
		if (m_RoughnessMap) {
			m_RoughnessMap->bind(currentTextureUnit++);
		}
		else {
			TextureLoader::getDefaultRoughness()->bind(currentTextureUnit++);
		}

		shader->setUniform("material.texture_ao", currentTextureUnit);
		if (m_AmbientOcclusionMap) {
			m_AmbientOcclusionMap->bind(currentTextureUnit++);
		}
		else {
			TextureLoader::getDefaultAO()->bind(currentTextureUnit++);
		}

		shader->setUniform("material.texture_displacement", currentTextureUnit);
		if (m_DisplacementMap) {
			shader->setUniform("hasDisplacement", true);
			shader->setUniform("minMaxDisplacementSteps", glm::vec2(m_ParallaxMinSteps, m_ParallelMaxSteps));
			shader->setUniform("parallaxStrength", m_ParallaxStrength);
			m_DisplacementMap->bind(currentTextureUnit++);
		}
		else {
			shader->setUniform("hasDisplacement", false);
		}

	}
```

 

### IBL Probe

基于光照探针生成IBL信息。

* 漫反射	

​	具体原理:[漫反射辐照 - LearnOpenGL CN (learnopengl-cn.github.io)](https://learnopengl-cn.github.io/07 PBR/03 IBL/01 Diffuse irradiance/)

​		在探针位置生成一个CubemapCaemra，从该点看向场景生成CubeMap，对CubeMap进行采样卷积，生成irradianceMap，

* 镜面反射

  具体原理:[镜面IBL - LearnOpenGL CN (learnopengl-cn.github.io)](https://learnopengl-cn.github.io/07 PBR/03 IBL/02 Specular IBL/)

  将镜面反射部分拆分成了两部分进行预计算，预滤波环境贴图与BRDF计算。		

  * 预滤波环境卷积

  ![image-20231126170402413](C++.assets\image-20231126170402413.png)

  这次卷积引入了粗糙度，由于跟视角关系有关，又做出了大胆的假设

  ```cpp
  vec3 N = normalize(w_o);
  vec3 R = N;
  vec3 V = R;
  ```

  基于蒙特卡洛方法，使用低差异序列生成蒙特卡洛样本向量，完成重要性采样，最终生成预过滤卷积环境贴图，完成了第一项积分的计算。

  * BRDF

​			![image-20231126170733874](C++.assets\image-20231126170733874.png)

将公式转化为F0 * A + B，A、B即为预先计算的BRDF贴图采样的结果，其输入是 n 和 ωo 的夹角，以及粗糙度，并将卷积的结果存储在纹理中。我们将卷积后的结果存储在 2D 查找纹理（Look Up Texture, LUT）中。最终完成应用。



### 前向渲染与延迟渲染

首先init渲染，生成环境Probe即完成IBL对应的预计算

* 前向渲染
  * 阴影深度图pass生成
  * forward PBR Lighting Pass
  * 后处理Pass
* 延迟渲染
  * geometryPass，第一遍渲染保存下来opaque物体的法线、世界位置、材质等信息
  * PreLighting Pass，预计算环境AO贴图，使用SSAO算法实现
  * Deffered PBR Lighting Pass，NDC_Plane Draw完成fragment lighting 的计算
  * forward PBR Lighting Pass渲染透明物体
  * 后处理Pass

* 

###  阴影

* 生成以光源为相机的深度贴图
* 在光照Pass中使用，完成PCF阴影，在深度中多次采样以达到一顶程度上阴影软化的效果。



### 后处理

#### Bloom(泛光)

![image-20231126154252607](C++.assets\image-20231126154252607.png)

是在hdr-》sdr之前进行的，里面的值仍可能>1

* 第一遍pass根据阈值提取出高亮部分
* 第二遍pass运用高斯模糊，模糊处理高亮部分，可以运用水平一次pass、竖直一次pass减少计算量
* 第三遍混合原图和模糊处理后的高亮部分，以达到bloom的效果

#### ChromaticAberration（色差处理）

![image-20231126154319279](C++.assets\image-20231126154319279.png)

是在sdr之后处理的，采用了一个快速实现，计算每个点的uv和中心点的uv(0.5, 0.5)的差值以确定偏移值，采样的时候分别通过偏移值采样不同点的r、g、b来实现色差效果

```glsl
vec2 diffFromCenter = TexCoords - vec2(0.5, 0.5);
vec2 offset = diffFromCenter * texel_size * intensity;

// Emulated how Unity handles their "fast" implementation
float r = texture2D(input_texture, TexCoords - (1 * offset)).r;
float g = texture2D(input_texture, TexCoords - (2 * offset)).g;
float b = texture2D(input_texture, TexCoords - (3 * offset)).b;
```

#### Vignette (晕影效果)



![image-20231126154619961](C++.assets\image-20231126154619961.png)

设定一默认颜色黑色，根据uv和中心点之间的距离，给不同的mix权重，在中心点mix就比较大，采样结果输入图像对应结果，对于比较远的边缘的点，mix结果比较小，黑色占比比较大，实现了这种晕影效果

```glsl
vec2 uv = TexCoords;
uv *= 1.0 - TexCoords.xy;
float vig = uv.x * uv.y * 15.0;
vig = pow(vig, intensity * 5.0);

FragColour = vec4(mix(color, sceneColour, vig), 1.0);
```

#### FXAA 后处理抗锯齿

* 找到锯齿的位置，通过采样一个范围内的点(四个角落)，判断最小值和最大值的差别，如果太大认为存在锯齿。
* 确定采样的方向,x方向渐变比较大说明锯齿在y方向上，y方向变化比较大说明锯齿在x方向。
* 进行模糊处理

####  SMAA 后处理抗锯齿

####  Film Grain(电影颗粒效果)

![image-20231126160306266](C++.assets\image-20231126160306266.png)

根据时间和uv生成一些随机颗粒，叠加到颜色中去，时间一直在更新，因此看起来有颗粒波动的效果。

	vec3 colour = texture2D(input_texture, TexCoords).rgb;
	
	float x = (TexCoords.x + 4) * (TexCoords.y + 4) * ((time + 1) * 10.0);
	vec4 grain = vec4(mod((mod(x, 13.0) + 1.0) * (mod(x, 123.0) + 1.0), 0.01) - 0.005) * intensity;
	
	FragColour = vec4(colour + grain.xyz, 1.0);

#### OpenGL 模拟水流

可以试想下，相机在水面以上，

反射效果：通过过在水面上方可以看到水面下方，水面以上物体（天空盒）会被水面反射看到；

折射效果：通过过在水面上方可以看到水面下方，可以看到水面下面的网格地面。

实现方式如下：从场景中多个视角渲染多个缓冲区，然后使用帧缓冲区中的结果作为纹理，融合到ADS光照水面。

* **反射效果实现**

![image-20231129194313376](C++.assets\image-20231129194313376.png)

反射纹理：反射相机与主相机在y轴的相反位置，同时反射相机向X轴方向倾斜，倾斜角度为主相机相反的倾斜角度。我们从反射相机的角度进行渲染，只渲染水面以上的物体（天空盒），最终渲染结果颜色对应Y坐标需要反转(1.0-tex.y)，不渲染水面底部及顶部纹理。

* **折射效果实现**

折射相机与主相机在相同位置，具有相同角度。使用和主相机相同的视图矩阵，折射纹理便是通过折射相机进行渲染，渲染透过水面可以看到的物体，（比如水底棋盘格）。

* **加一些扰动**

在采样reflect与refract texture的时候，利用dudv纹理加入一些扰动，让结果看起来更加真实

```cpp
// Apply offset to the sampled coords for the refracted & reflected texture
vec2 distortion;


vec2 totalDistortion;
if (clearWater) {
    distortion = vec2(0.0, 0.0);
    totalDistortion = vec2(0.0, 0.0);
}
else {
    distortion = (texture(dudvWaveTexture, vec2(planeTexCoords.x + waveMoveFactor, planeTexCoords.y)).rg * 2.0 - 1.0) * 0.1; // Unpack the dudv map
    distortion = planeTexCoords + vec2(distortion.x, distortion.y + waveMoveFactor);
    totalDistortion = (texture(dudvWaveTexture, distortion).rg * 2.0 - 1.0) * waveStrength * dampeningEffect; // Unpack the dudv map and multiply by strength
}
reflectCoords += totalDistortion;
reflectCoords = clamp(reflectCoords, 0.0, 1.0);
refractCoords += totalDistortion;
refractCoords = clamp(refractCoords, 0.0, 1.0);
vec4 reflectedColour = texture(reflectionTexture, reflectCoords);
vec4 refractedColour = texture(refractionTexture, refractCoords);

```

* **计算高光**

利用normal map，与入射角，计算反射角与太阳光照夹角，计算一个简略的高光

```glsl
vec3 normal;
	if (clearWater) {
		normal = vec3(0.0, 1.0, 0.0);
	}
	else {
		normal = texture(normalMap, distortion).rgb;
		// Assumes the normal of the water plane is always (0, 1, 0) 
		// so the the y component for the normal will never be negative
		normal = normalize(vec3(normal.r * 2.0 - 1.0, normal.g * waterNormalSmoothing, normal.b * 2.0 - 1.0)); 
	}

	vec3 viewVec = normalize(fragToView);
	float fresnel = dot(viewVec, vec3(0.0, 1.0, 0.0)); // TODO: Should use sampled normal

	// Direct Specular light highlights (TODO: Actually make this work for multiple directional lights, right now it requires there to be one directional light and only uses one)
	vec3 reflectedVec = reflect(normalize(dirLights[0].direction), normal);
	float specular = max(dot(reflectedVec, viewVec), 0.0);
	specular = pow(specular, shineDamper);
	vec3 specHighlight = dirLights[0].lightColor * specular * reflectivity * dampeningEffect;
```



* **透明度设置**

根据水面和水底之间的距离来设置透明度，如果离得太远，就设置为1.0

```glsl
float near = nearFarPlaneValues.x;
float far = nearFarPlaneValues.y;
float depth = texture(refractionDepthTexture, refractCoords).r;
float cameraToSurfaceFloorDistance = 2.0 * near * far / (far + near - (2.0 * depth - 1.0) * (far - near));
depth = gl_FragCoord.z;

float cameraToWaterDistance = 2.0 * near * far / (far + near - (2.0 * depth - 1.0) * (far - near));
float waterDepth = cameraToSurfaceFloorDistance - cameraToWaterDistance;
float dampeningEffect = clamp(waterDepth / dampeningEffectStrength, 0.0, 1.0);

//calculate
//....
//...

FragColour.a = dampeningEffect;
```

### Unity

* **Unity SRP原理**

#### **CBDL**

分簇延时光照是一种流行的光照计算优化策略，允许海量的同屏光照，CBDL将相机视锥体分为了多簇，并为每簇分配若干的有效光源，可以避免大量的无效光照计算，分簇光照分为两步，预处理和着色。

* 预处理阶段
  * 分割相机视锥体，生成若干个Cluster
  * 对于每个Cluster，便利所有光源求焦点，得到影响该cluster的有效光源列表
* 着色阶段
  * 根据像素坐标生成该像素所属的Cluster，遍历该cluster的"有效光源列表"，逐一计算光照，每个光源操作是相同的，知识簇和光源的数据不同，符合SIMD并行思想，使用Compute Shader并行进行分簇和光源分配

准备了四个Buffer，分别存放Cluster信息表(世界坐标八个点的坐标)，光源信息表，光源分配结果表以及光源分配索引表，所有Buffer都是一维的，使用的时候根据compute shader thread id三维坐标转换为一维坐标再读取数据

![image-20231128154553347](C++.assets\image-20231128154553347.png)

光源分配索引表（assignTable）中每个元素对应一个 Cluster，每个元素存储了 start 和 count，表示该 Cluster 受到哪些光源的影响。

在光源分配结果表（lightAssignBuffer）的 [start, start+count) 区间存储的是这些光源的 id，即光源在 lightBuffer 中的下标，所以光源分配结果表以 sizeof uint 为 stride

具体的索引过程如下。首先通过 Cluster ID 查 assignTable，然后遍历 lightAssignBuffer 获取灯光 ID，再根据灯光 ID 查 lightBuffer 获取灯光信息：

![image-20231128154709274](C++.assets\image-20231128154709274.png)

* 分簇

  每一个簇都可以用粗暴的8个点表示

![image-20231128155149549](C++.assets\image-20231128155149549.png)



分割方法如下。这里通过 SV_GroupThreadID 得到 Cluster 的 xy 索引，通过 SV_GroupID 得到 z 方向的索引，组成三维 Cluster ID，用（i，j，k）表示。然后：

1. 通过 i、j 得到 NDC 空间下该 Cluster 的 xy 二维 Rect
2. 分别用 0 和 1 做深度，将 Rect 反投影得到世界空间下近、远截面的梯形台
3. 通过 k 对梯形台进行切分，截取我们要的第 k 级 cluster
4. 将结果保存到 Compute Buffer

步骤比较简单，但我的语言表达能力捉急，于是大概流程图如下：

![image-20231128155235852](C++.assets\image-20231128155235852.png)

分簇结果绘制出来大概是这样：

![image-20231128155306108](C++.assets\image-20231128155306108.png)

* 传递光源信息

Compute buffer允许用户使用SetDada在CPU端设置数据。在每一帧通过Resources.FindObjectsOfType 获取全部的光源列表，然后更新光源信息到 Buffer，同时向 Shader 传递有效的光源数量。

```cpp
public void UpdateLightBuffer(Light[] lights) {
    PointLight[] plights = new PointLight[maxNumLights];
    int cnt = 0;
    for (int i = 0; i < lights.Length; ++i) {
        if (lights[i].type != LightType.Point) {
            continue;
        }
        PointLight pl;
        pl.color = new Vector3(lights[i].color.r, lights[i].color.g, lights[i].color.b);
        pl.intensity = lights[i].intensity;
        pl.position = lights[i].transform.position;
        pl.radius = lights[i].range;
        plights[cnt++] = pl;
    }
    lightBuffer.SetData(plights);

    //传递光源数量
    lightAssignCS.SetInt("_numLights", cnt);
}
```

也可以使用 Unity SRP API 提供的裁剪方法对视锥体外的光源进行裁剪，传递裁剪过后的光源列表能有效提高后续求交操作的效率。注意这里裁剪之后返回的是 VisuableLight 而不是 Light 对象，要重载多一个方法：

```cpp
public void UpdateLightBuffer(VisibleLight[] lights)
{
    PointLight[] plights = new PointLight[maxNumLights];
    int cnt = 0;
    for (int i = 0; i < lights.Length; ++i)
    {
        var vl = lights[i].light;
        if (vl.type != LightType.Point)
        {
            continue;
        }
        PointLight pl;
        pl.color = new Vector3(vl.color.r, vl.color.g, vl.color.b);
        pl.intensity = vl.intensity;
        pl.position = vl.transform.position;
        pl.radius = vl.range;
        plights[cnt++] = pl;
    }
    lightBuffer.SetData(plights);

    //传递光源数量
    lightAssignCS.SetInt("_numLights", cnt);
}
```

* 光源求交计算

这部分计算也是有Compute Shader承担，求交判断box的八个点在不在球的内部，只要有一个点在，就记录到该Cluster对应的结果存放表中

```cpp
[numthreads(16,16,1)]
void LightAssign(
    uint3 gtid : SV_GROUPTHREADID,
    uint3 gid : SV_GROUPID)
{
    // cluster ID
    uint i = gtid.x, j = gtid.y, k = gid.x;
    uint3 cluster_3D = uint3(i, j, k);
    uint clusterId_1D = Index3DTo1D(cluster_3D);

    ClusterBox box = _clusterBuffer[clusterId_1D];

    //in _lightAssignBuffer's  index
    uint startIndex = clusterId_1D * _maxNumLightsPerCluster;
    uint endIndex = startIndex;

    //search the intersect with the light
    for(int lid = 0; lid < _numLights; ++lid){
        PointLight pl = _lightBuffer[lid];
        if(ClusterLightIntersect(box, pl)){
            _lightAssignBuffer[endIndex++] = uint(lid);
        }
    }

    //write the res
    LightIndex idx;
    idx.count = endIndex - startIndex;
    idx.start = startIndex;
    _assignTable[clusterId_1D] = idx;
}
```

* 光照计算的应用

根据像素位置求出所在的clusterID，根据光照结果计算表求出在该Cluster下，光照的贡献结果，点源衰减计算套取了一个衰减公式：

![image-20231128161047635](C++.assets\image-20231128161047635.png)

```cpp
//计算Cluster Based Lighting
uint x = floor(uv.x * _numClusterX);
uint y = floor(uv.y * _numClusterY);
uint z = floor((1 - d_lin) * _numClusterZ); //z是反的 dx

uint3 clusterId_3D = uint3(x, y, z);
uint clusterId_1D = Index3DTo1D(clusterId_3D);
LightIndex lightIndex = _assignTable[clusterId_1D];

int start = lightIndex.start;
int end = lightIndex.start + lightIndex.count;
for(int j = start; j < end; ++j){
    uint lightId = _lightAssignBuffer[j]; //灯光ID
    PointLight lit = _lightBuffer[lightId]; //根据id查找灯光表

    L = normalize(lit.position - worldPos.xyz);
    radiance = lit.color;

    //灯光衰减
    float dis = distance(lit.position, worldPos.xyz);
    float d2 = dis * dis;
    float r2 = lit.radius * lit.radius;
    float dying = saturate(1 - (d2 / r2) * (d2 / r2));
    dying *= dying;

    color += PBR(N, V, L, albedo, radiance, roughness, metallic) * lit.intensity * dying;
}
```

* **CBDL有光源比较小，在Box里面怎么办？**

两种情况，第一种情况

1. 光源在簇的外部，但是仍然影响着该簇，那么可以根据距离来判断，光源和簇的Box八个点最小距离小于影响半径，那么加进去！
2. 光源在簇的内部，直接根据位置来筛选，在里面的话光源的xyz一定位于box的min(xyz)~max(xyz)之间

* **如何加速求交**

**BVH包围盒啊！！**





#### 剔除

*  **视锥剔除**

demo完成对同一个物体的剔除，该物体数量很多

1. 求出世界空间剔除的六个面

2. 向compute shader中传入物体的坐标变换矩阵

3. 完成坐标变换，将物体从局部空间转换到世界空间，通过包围盒和平面的位置判断是否相交。

4. 具体判断过程：

   ![image-20231114192014447](C++.assets\image-20231114192014447.png)



如图：判断OA和法向量n的关系，如果夹角小于90°说明该点在平面外，如果包围盒的所有点都在某个面的外部，那么该物体就在平面的外部，不在视锥内，则不需要绘制。

* **遮挡剔除hiz**

Hiz剔除主要是利用mipmap的思想，先用包围盒包裹住一个物体，如果包围盒内的所有物体的深度都比depthTexture上的大，那么说明包围盒所有物体都被遮挡住了，这个时候不需要往shader中传递数据，那么问题就转换为了如何计算一个范围内物体的深度，自然就利用到了mipmap，生成深度图的时候，手动写一下depth shader的代码，将上一层的mipmap传入shader，生成这一层的mipmap

```hlsl
    float4 depth;
    float offset = _MainTex_TexelSize.x / 2;
    depth.x = tex2D(_MainTex, uv);
    depth.y = tex2D(_MainTex, uv + float2(0, offset));
    depth.z = tex2D(_MainTex, uv + float2(offset, 0));
    depth.w = tex2D(_MainTex, uv + float2(offset, offset));
#if defined(UNITY_REVERSED_Z)
	return min(min(depth.x, depth.y), min(depth.z, depth.w));
#else
	return max(max(depth.x, depth.y), max(depth.z, depth.w));
#endif
```

mipmap存储可见的距离摄像机最远的位置的深度值，然后进行Hiz剔除compute shader,在ndc空间重新生成一个包围盒，计算出对应位置的uv坐标，可以计算出左下角右上角，计算出应该读取的mipmap level以及mipmap size大小，将包围盒映射到Mipmap level Texture的四个点,取最小值,即可完成范围查询，并根据深度信息剔除不必要的数据。

* **如果直接用摄像机的深度图怎么办？摄像机的深度图一般不是2的n次方，但是Hiz深度图一般又都是2的n次方，这种情况怎么处理？**

1. UV再映射？深度图映射到2的n次方的纹理坐标空间？
2. ？？？？？？



# 游戏引擎

* **说一说你对游戏引擎的理解**

我认为游戏引擎就是一个软件框架，用于简化和加速游戏开发的过程，每一层都有特定的含义，根据现代游戏引擎开发的基础，一般自顶向下有工具层（各种编辑器的操作）、功能层（渲染、动画、物理、交互、脚本AI、状态机）、资源层（文件和数据）、核心层（复用的底层代码、数学库等等）、平台层（不同平台用户）。



# 逻辑题

* **一个是两种药片，每种有两个，一个人需要早上吃两种药片各一个，现在这四个药片混在一起了这个人什么方法吃。**

把所有的4颗药丸都切开成相等的两半，然后早上和晚上，分别吃掉每颗药丸的一半



* **一共1000瓶酒，其中一瓶有毒。如果一只老鼠喝了有毒的酒，会在一天之后死亡，那么如果给你一天时间，然你判定哪瓶酒有毒，至少需要几只老鼠？**

答案是10只。这个需要使用二进制编码来解决，1000瓶酒至少需要10位二进制数来进行编码。然后取十只杯子分别代表这是个二进制数的十个位，分别将1000瓶酒倒入其编码为1的对应的杯子中。取十个老鼠分别喝十个杯子中的酒，一天之后，就可以根据喝哪些杯子的老鼠死掉来确定出有毒的那瓶酒的编码，从而确定哪瓶酒有毒。其根据就是只有有毒酒的编码对应的毒死老鼠的杯子位置。这个题目就是利用了二进制编码的一些特性。

# 常见算法题

## 排序

* **快速排序**

```cpp
class Solution {
public:
    void quickSort(vector<int>& vec, int left, int right) {
        if (left >= right)
            return;
        int l = left;
        int r = right;
        
        int index = rand() % (right - left + 1);
        swap(vec[left + index], vec[left]);
        int privot = vec[left++];
        

        while (left <= right)
        {
            while (left <= right && vec[right] >= privot) {
                --right;
            }

            while (left <= right && vec[left] <= privot)
            {
                ++left;
            }
            if (left < right) {
                swap(vec[left], vec[right]);
            }
        }
        swap(vec[l], vec[right]);
        
        int leftPivot = right - 1;
        int rightPivot = right + 1;
        // 优化二
        while(leftPivot >= l && vec[leftPivot] == vec[right]) leftPivot--;
        while(rightPivot <= r && vec[rightPivot] == vec[right]) rightPivot++;

        quickSort(vec, l, leftPivot);
        quickSort(vec, rightPivot, r);
    }
    vector<int> sortArray(vector<int>& nums) {
        quickSort(nums, 0, nums.size() - 1);
        return nums;
    }
};
```

* **归并排序**

```cpp
class Solution {
    vector<int> tmp;
    void mergeSort(vector<int>& nums, int l, int r) {
        if (l >= r) return;
        int mid = (l + r) >> 1;
        mergeSort(nums, l, mid);
        mergeSort(nums, mid + 1, r);
        int i = l, j = mid + 1;
        int cnt = 0;
        while (i <= mid && j <= r) {
            if (nums[i] <= nums[j]) {
                tmp[cnt++] = nums[i++];
            }
            else {
                tmp[cnt++] = nums[j++];
            }
        }
        while (i <= mid) {
            tmp[cnt++] = nums[i++];
        }
        while (j <= r) {
            tmp[cnt++] = nums[j++];
        }
        for (int i = 0; i < r - l + 1; ++i) {
            nums[i + l] = tmp[i];
        }
    }
public:
    vector<int> sortArray(vector<int>& nums) {
        tmp.resize((int)nums.size(), 0);
        mergeSort(nums, 0, (int)nums.size() - 1);
        return nums;
    }
};
```

* **堆排序**



# 开放题





# 自己的面经

## FunPlus游戏引擎

### 一面(日常实习)

* **图形**

1. **阴影的原理**
2. **PCSS**
3. **常见抗锯齿方案原理(SSAA、MSAA、FXAA)**
4. **CSM划分的特点，Unity怎么划分？包围盒怎么计算的？**
5. **Shader dudv原理了解吗**

​	这两条指令用于对指定的寄存器，求其值在临近像素上的变化率，因为纹理坐标的梯度可以用来确定纹理当前被缩放的程度，可用该值来计算Mip层，另外它也可以用来计算Texel的跨越Size，由此求得正确的过滤宽度，从而纠正通常的线性过滤在远处由于过滤宽度错误而产生的失真。

6. **Shader渲染实际是一块一块的，有了解吗？**

**偏导计算**
在三角形光栅化是，GPU 都是 block 片段来计算光栅化的。偏导计算于是由这block之间的片段的值来计算的；dFdx 计算并返回的是右边的片段减去左边的片段的值，而 dFdy 是有上减去下的值。查看下图的格式显示的就 block 中对应的 (x, y) 屏幕坐标上的片段。

![image-20231208182344966](C++.assets\image-20231208182344966.png)



8. **SSAO算法原理？前向渲染实现SSAO？**
8. **根据深度图如何还原出法线图？**

[【知识补充】深度信息还原位置和法线 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/367257314)

对计算好的AO图进行模糊，消除里面的噪点

10. **TBN矩阵坐标轴的含义？怎么推导的？**

11. **Hiz遮挡剔除怎么实现mipmap层级确定？降采样怎么实现的？**

将AABB包围盒投影到NDC空间，求出来对应的距离范围，再和原始图像相乘，求出来覆盖的区域，取log2求出来mipmap等级。

```cpp
// 计算中心和 bounding box 的宽高
float4 center = float4(0,0,0,1);
float xmax=-1, ymax=-1, xmin=1, ymin=1, zmax=-1, zmin=1;
for(int i=0; i<8; i++)
{
    // to ndc space
    float4 ndcBounds = mul(_vpMatrix, _bounds[i]);
    ndcBounds.xyz /= ndcBounds.w;
    center.xyz += ndcBounds.xyz;

    xmax = max(xmax, ndcBounds.x);
    ymax = max(ymax, ndcBounds.y);
    xmin = min(xmin, ndcBounds.x);
    ymin = min(ymin, ndcBounds.y);
    zmax = max(zmax, ndcBounds.z);
    zmin = min(zmin, ndcBounds.z);
}
center.xyz /= 8;
float2 uv = center.xy * 0.5 + 0.5;

// 计算 mip 等级
float boxSize = clamp(max(xmax - xmin, ymax - ymin) * 0.5, 0, 1);
int lod = clamp(floor(log2(boxSize * _size)), 0, 15);

uv *= _size / pow(2, lod);
float d = _hizBuffer.mips[lod][int2(uv)].r;
```



* C++

1. **内存分区**

堆区、栈区、全局区\静态区、代码区

2. **堆区和栈区谁的分配速度更快？为什么？**

栈区更快。

栈：只要栈的剩余空间大于所申请空间，系统将为程序提供内存，否则将报异常提示栈溢出。
堆：首先应该知道操作系统有一个记录空闲内存地址的链表，当系统收到程序的申请时，会遍历该链表，寻找第一个空间大于所申请空间的堆结点，然后将该结点从空闲结点链表中删除，并将该结点的空间分配给程序。
对于大多数系统，会在这块内存空间中的首地址处记录本次分配的大小，这样，代码中的 delete 语句才能正确的释放本内存空间。

3. **内存碎片(内部碎片、外部碎片)**

### 二面

* **图形**

1. **介绍一下光栅化渲染管线**

2. **cook-torrance模型**

3. **BRDF漫反射项的常数项怎么确定的？($f_d$)**

   ![image-20231213193147837](C++.assets/image-20231213193147837.png)

4. **F0的确定，非金属为什么是是0.04？**

   ```cpp
   // calculate reflectance at normal incidence; if dia-electric (like plastic) use F0 
   // of 0.04 and if it's a metal, use the albedo color as F0 (metallic workflow)    
   vec3 F0 = vec3(0.04); 
   F0 = mix(F0, albedo, metallic);
   ```

5. **BRDF镜面项，FDG各项的含义**

   * D：表示的是微表面结构中法线分布函数，它描述了光线以多大的概率在不同方向上的散射。业界常用的法线分布函数是GGX，具有更好的高光长尾

   ![image-20231212153920593](C++.assets/image-20231212153920593.png)

   * F：菲涅尔项，表示的是光线在材质表面与介质之间的反射和折射的行为，掠视金属时反射较多的光而俯视时反射光较少

   ![image-20231212154008402](C++.assets/image-20231212154008402.png)

   * G：表示的是几何遮蔽的情况，返回一个未被遮蔽的表面的百分比，常用GGX模型，通过史密斯法叠加入射和出射两个方向。

6. **IBL原理**

7. **IBL高光项，mipmap和roughness对应关系**

   ```cpp
   //learnopengl
   mip = linearRoughness * maxMip;
   //unity
   mip = linearRoughness * (1.7f - 0.7f * linearRoughness) * maxMip
   ```

   线性分布不合理，可以让前面的mipmap更加精细一点，后面的mipmap更加粗糙一点。

   绿色的opengl线性分布，蓝色的是unity中的分布，unity中的更加合理。

   ![image-20231213204822257](C++.assets/image-20231213204822257.png)

8. **CSM原理**

   

* **C++**

1. **C++为什么引入智能指针？**

   贯彻RAII思想，帮助我们程序员做好内存管理工作，防止内存泄漏。

2. **unique_ptr怎么保证唯一性？**

   禁止拷贝构造和赋值构造

3. **unique_ptr和裸指针的性能上有什么差异？**

   * **内存管理开销：** `std::unique_ptr` 通常包含一个指向堆上对象的指针和一个额外的控制块，用于管理资源的所有权。这可能导致 `std::unique_ptr` 对象相较于裸指针占用更多的内存。然而，现代编译器通常会进行优化，使得这种开销相对较小。

   * **构造和销毁开销：** 创建和销毁 `std::unique_ptr` 对象可能涉及更多的工作，因为它需要在构造和析构时管理资源。相较之下，裸指针的构造和销毁开销较小。但这种差异通常在实际应用中不是性能瓶颈。

   * **函数调用开销：** 将 `std::unique_ptr` 传递给函数可能会涉及更多的指针复制，因为 `std::unique_ptr` 是非拷贝可传递（move-only）的。这意味着在函数调用中可能会执行资源所有权的转移。对比之下，裸指针的传递只涉及指针值的复制。

   * **异常处理开销：** `std::unique_ptr` 提供了异常安全性，当异常发生时，会自动释放其管理的资源。这可能导致与裸指针相比更多的开销。但这种开销通常是可以接受的，尤其是考虑到异常安全性的好处。

4. **move函数的作用？**

   `std::move` 并不实际执行任何移动操作，而是将一个左值强制转换为右值引用，从而允许使用移动语义。

5. **什么情况下需要用到forward转发？**

   `std::forward` 主要用于在泛型编程中进行完美转发（perfect forwarding），以保留原始参数的值类别（是左值还是右值）并正确传递给其他函数。下面是一些需要使用 `std::forward` 的常见情况：

6. **move和forward有性能上的损失吗？**

   应该没有吧，static_cast用的是，编译器进行的，运行时应该没有损失。

7. **static_cast有性能损失嘛？**

   `static_cast` 是 C++ 中的一种类型转换操作符，它通常在编译时进行，并且没有运行时开销。在理论上，`static_cast` 本身不应该引入性能损失，因为它只是进行了静态类型转换。

8. **shared_ptr的实现除了引用计数，还有什么东西？**

   `std::shared_ptr` 的实现还包含以下主要元素：

   **控制块（Control Block）：** 在堆上分配的动态对象通常由一个控制块来管理。控制块包含了引用计数以及额外的信息，用于协调多个 `std::shared_ptr` 共享同一块动态内存。控制块一般包含以下信息：

   - 引用计数：用于记录有多少个 `std::shared_ptr` 共享同一块内存。
   - 原始对象的指针：指向实际的动态分配的对象。
   - 删除器（Deleter）：负责在引用计数减为零时释放资源的函数或函数对象。

9. **shared_ptr性能问题可能出现在哪？**

   * **多线程竞争：** `std::shared_ptr` 的引用计数是原子操作，但在极端的多线程环境中，可能存在竞争条件。频繁地创建和销毁大量的 `std::shared_ptr` 实例，特别是在多线程环境下，可能导致引用计数的频繁增减，从而影响性能。
   * **析构函数：**当给shared_ptr指针指向另一个地方，原来的内存可能被析构，比较耗时。
   * **循环引用：** 如果存在循环引用（两个或多个 `std::shared_ptr` 彼此引用），可能导致对象永远无法被释放，从而导致内存泄漏。虽然 `std::shared_ptr` 引入了 `std::weak_ptr` 来处理循环引用问题，但仍需要小心使用，以避免潜在的性能和内存泄漏问题。

10. **C++开发中你遇到的最有趣的东西是什么？**

    * bool没写返回值，返回的值是不确定的，可能与预想不太一样。
    * volitile防止编译器优化，cuda调试
